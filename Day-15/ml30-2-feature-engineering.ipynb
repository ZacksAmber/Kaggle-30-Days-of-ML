{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Statistics\nimport pandas as pd\nimport numpy as np\nimport math as mt\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# Data Preprocessing - Standardization, Encoding, Imputation\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\n\n\n# Data Preprocessing - Feature Engineering\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.decomposition import PCA\n\n# Data Preprocessing - ML Pipelines\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# ML - Modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# ML - Evaluation\nfrom sklearn.model_selection import cross_val_score\n\n# ML - Tuning\nfrom sklearn.model_selection import GridSearchCV","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-24T22:02:31.960471Z","iopub.execute_input":"2021-08-24T22:02:31.960824Z","iopub.status.idle":"2021-08-24T22:02:31.975769Z","shell.execute_reply.started":"2021-08-24T22:02:31.960791Z","shell.execute_reply":"2021-08-24T22:02:31.974847Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Read train dataset\ntrain_data = pd.read_csv('../input/30days-folds/train_folds.csv')\ntest_data = pd.read_csv('../input/30-days-of-ml/test.csv')\nsample_submission = pd.read_csv('../input/30-days-of-ml/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T22:26:54.699527Z","iopub.execute_input":"2021-08-24T22:26:54.699886Z","iopub.status.idle":"2021-08-24T22:26:56.502962Z","shell.execute_reply.started":"2021-08-24T22:26:54.699855Z","shell.execute_reply":"2021-08-24T22:26:56.502137Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"%%time\n# No Standardization\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    #print(\"encoding\")\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    #print(\"training\")\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model.fit(X_train, y_train)\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:31:02.267085Z","iopub.execute_input":"2021-08-24T21:31:02.267478Z","iopub.status.idle":"2021-08-24T21:31:24.811144Z","shell.execute_reply.started":"2021-08-24T21:31:02.267442Z","shell.execute_reply":"2021-08-24T21:31:24.810208Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"0 0.7245705537554137\n1 0.7242510333821858\n2 0.7270667092065692\n3 0.7268359229595335\n4 0.7257178555909586\n0.7256884149789322 0.0011430674400777338\nCPU times: user 22.6 s, sys: 771 ms, total: 23.3 s\nWall time: 22.5 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# With Standardization\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Preprocessing - Standardization\n    scaler = StandardScaler()\n    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n    X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n    X_test[num_cols] = scaler.transform(X_test[num_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:31:24.812699Z","iopub.execute_input":"2021-08-24T21:31:24.813254Z","iopub.status.idle":"2021-08-24T21:31:47.802759Z","shell.execute_reply.started":"2021-08-24T21:31:24.813211Z","shell.execute_reply":"2021-08-24T21:31:47.801846Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"0 0.7241755479182882\n1 0.7241138968948254\n2 0.7267386816038165\n3 0.7268357864120136\n4 0.725667388462628\n0.7255062602583143 0.001185068397378747\nCPU times: user 22.9 s, sys: 666 ms, total: 23.5 s\nWall time: 23 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# With Normalization\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n\n    # Preprocessing - Normalizatino\n    normalizer = Normalizer()\n    X_train[num_cols] = normalizer.fit_transform(X_train[num_cols])\n    X_valid[num_cols] = normalizer.transform(X_valid[num_cols])\n    X_test[num_cols] = normalizer.transform(X_test[num_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:42:54.460834Z","iopub.execute_input":"2021-08-24T21:42:54.461192Z","iopub.status.idle":"2021-08-24T21:43:17.569538Z","shell.execute_reply.started":"2021-08-24T21:42:54.461157Z","shell.execute_reply":"2021-08-24T21:43:17.568484Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"0 0.7388116889933081\n1 0.738024917853026\n2 0.7409186250111057\n3 0.7396434860172969\n4 0.740614097083746\n0.7396025629916967 0.0010836365642808196\nCPU times: user 23 s, sys: 829 ms, total: 23.8 s\nWall time: 23.1 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# With Standardization + Normalization\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n\n    # Preprocessing - Standardization\n    scaler = StandardScaler()\n    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n    X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n    X_test[num_cols] = scaler.transform(X_test[num_cols]) # Q. The last transform\n    \n    # Preprocessing - Normalizatino\n    normalizer = Normalizer()\n    X_train[num_cols] = normalizer.fit_transform(X_train[num_cols])\n    X_valid[num_cols] = normalizer.transform(X_valid[num_cols])\n    X_test[num_cols] = normalizer.transform(X_test[num_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:44:15.294985Z","iopub.execute_input":"2021-08-24T21:44:15.295451Z","iopub.status.idle":"2021-08-24T21:44:38.737812Z","shell.execute_reply.started":"2021-08-24T21:44:15.295406Z","shell.execute_reply":"2021-08-24T21:44:38.736862Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"0 0.7347795628127367\n1 0.7354956594532642\n2 0.7363710377512285\n3 0.7369826624019941\n4 0.7375458010938692\n0.7362349447026185 0.0009960548860008741\nCPU times: user 23.4 s, sys: 694 ms, total: 24.1 s\nWall time: 23.4 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# With Standardization\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Preprocessing - Standardization\n    scaler = StandardScaler()\n    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n    X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n    X_test[num_cols] = scaler.transform(X_test[num_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Log transformation + Tuning\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfor col in num_cols:\n    train_data[col] = np.log1p(train_data[col])\n    test_data[col] = np.log1p(test_data[col])\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nprint('You need to reset dataframe!')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:47:08.741473Z","iopub.execute_input":"2021-08-24T21:47:08.741828Z","iopub.status.idle":"2021-08-24T21:47:37.645747Z","shell.execute_reply.started":"2021-08-24T21:47:08.741796Z","shell.execute_reply":"2021-08-24T21:47:37.644785Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"0 0.7187408168536737\n1 0.7184891153933534\n2 0.7204486544876513\n3 0.7204357985914301\n4 0.7191577004325692\n0.7194544171517355 0.0008343489787694719\nCPU times: user 28.8 s, sys: 853 ms, total: 29.6 s\nWall time: 28.9 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# polynomial features + Tuning\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\npoly = PolynomialFeatures(degree=2, \n                          interaction_only=True, # If true, only interaction features are produced: features that are products of at most degree distinct input features (so not x[1] ** 2, x[0] * x[2] ** 3, etc.).\n                          include_bias=False)\ntrain_poly = poly.fit_transform(train_data[num_cols])\ntest_poly = poly.fit_transform(test_data[num_cols])\n\ndf_train_poly = pd.DataFrame(train_poly, columns=[f\"poly_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"poly_{i}\" for i in range(test_poly.shape[1])])\n\ntrain_data = pd.concat([train_data, df_train_poly], axis=1)\ntest_data = pd.concat([test_data, df_test_poly], axis=1)\n\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nprint('You need to reset dataframe!')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T22:20:02.277023Z","iopub.execute_input":"2021-08-24T22:20:02.277410Z","iopub.status.idle":"2021-08-24T22:20:55.762472Z","shell.execute_reply.started":"2021-08-24T22:20:02.277378Z","shell.execute_reply":"2021-08-24T22:20:55.761626Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"0 0.7205875231523107\n1 0.7204528173429748\n2 0.7224474858735078\n3 0.7217891599660472\n4 0.7210479916863682\n0.7212649956042417 0.0007534885090919923\nYou need to reset dataframe!\nCPU times: user 52.2 s, sys: 4 s, total: 56.2 s\nWall time: 53.5 s\n","output_type":"stream"}]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2021-08-24T22:29:42.845424Z","iopub.execute_input":"2021-08-24T22:29:42.845783Z","iopub.status.idle":"2021-08-24T22:29:42.932702Z","shell.execute_reply.started":"2021-08-24T22:29:42.845746Z","shell.execute_reply":"2021-08-24T22:29:42.931612Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"       cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9  ...     cont4  \\\n0         B    B    B    C    B    B    A    E    E    I  ...  0.476739   \n1         A    B    A    C    B    C    A    E    C    H  ...  0.285509   \n2         B    A    A    A    B    B    A    E    D    K  ...  0.697272   \n3         B    B    A    C    B    D    A    E    A    N  ...  0.719306   \n4         B    B    A    C    B    C    A    E    C    F  ...  0.313032   \n...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n199995    B    A    A    C    B    D    A    E    E    I  ...  0.287454   \n199996    B    A    A    C    B    B    A    E    C    F  ...  0.794881   \n199997    A    B    B    C    B    B    A    E    C    I  ...  0.514487   \n199998    A    A    A    C    B    D    A    D    A    F  ...  0.286144   \n199999    A    A    A    C    B    D    A    E    A    J  ...  0.286755   \n\n           cont5     cont6     cont7     cont8     cont9    cont10    cont11  \\\n0       0.376350  0.337884  0.321832  0.445212  0.290258  0.244476  0.087914   \n1       0.860046  0.798712  0.835961  0.391657  0.288276  0.549568  0.905097   \n2       0.683600  0.404089  0.879379  0.275549  0.427871  0.491667  0.384315   \n3       0.777890  0.730954  0.644315  1.024017  0.391090  0.988340  0.411828   \n4       0.431007  0.390992  0.408874  0.447887  0.390253  0.648932  0.385935   \n...          ...       ...       ...       ...       ...       ...       ...   \n199995  0.543800  0.682378  1.028978  1.022741  0.683903  0.877273  0.532410   \n199996  0.432778  0.389775  0.359871  0.550013  0.492082  0.202295  0.416875   \n199997  0.060997  0.171741  0.317185  0.150340  0.122109  0.390524  0.334026   \n199998  1.061710  0.819811  0.901241  0.555339  0.844315  0.894193  0.794102   \n199999  1.065725  0.687682  0.654738  0.574575  0.617467  0.694336  0.745698   \n\n          cont12    cont13  \n0       0.301831  0.845702  \n1       0.850684  0.693940  \n2       0.376689  0.508099  \n3       0.393585  0.461372  \n4       0.370401  0.900412  \n...          ...       ...  \n199995  0.605397  0.884581  \n199996  0.406205  0.758665  \n199997  0.378987  0.839416  \n199998  0.844279  0.890473  \n199999  0.568525  0.783568  \n\n[200000 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat0</th>\n      <th>cat1</th>\n      <th>cat2</th>\n      <th>cat3</th>\n      <th>cat4</th>\n      <th>cat5</th>\n      <th>cat6</th>\n      <th>cat7</th>\n      <th>cat8</th>\n      <th>cat9</th>\n      <th>...</th>\n      <th>cont4</th>\n      <th>cont5</th>\n      <th>cont6</th>\n      <th>cont7</th>\n      <th>cont8</th>\n      <th>cont9</th>\n      <th>cont10</th>\n      <th>cont11</th>\n      <th>cont12</th>\n      <th>cont13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>E</td>\n      <td>E</td>\n      <td>I</td>\n      <td>...</td>\n      <td>0.476739</td>\n      <td>0.376350</td>\n      <td>0.337884</td>\n      <td>0.321832</td>\n      <td>0.445212</td>\n      <td>0.290258</td>\n      <td>0.244476</td>\n      <td>0.087914</td>\n      <td>0.301831</td>\n      <td>0.845702</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>C</td>\n      <td>B</td>\n      <td>C</td>\n      <td>A</td>\n      <td>E</td>\n      <td>C</td>\n      <td>H</td>\n      <td>...</td>\n      <td>0.285509</td>\n      <td>0.860046</td>\n      <td>0.798712</td>\n      <td>0.835961</td>\n      <td>0.391657</td>\n      <td>0.288276</td>\n      <td>0.549568</td>\n      <td>0.905097</td>\n      <td>0.850684</td>\n      <td>0.693940</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>E</td>\n      <td>D</td>\n      <td>K</td>\n      <td>...</td>\n      <td>0.697272</td>\n      <td>0.683600</td>\n      <td>0.404089</td>\n      <td>0.879379</td>\n      <td>0.275549</td>\n      <td>0.427871</td>\n      <td>0.491667</td>\n      <td>0.384315</td>\n      <td>0.376689</td>\n      <td>0.508099</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>C</td>\n      <td>B</td>\n      <td>D</td>\n      <td>A</td>\n      <td>E</td>\n      <td>A</td>\n      <td>N</td>\n      <td>...</td>\n      <td>0.719306</td>\n      <td>0.777890</td>\n      <td>0.730954</td>\n      <td>0.644315</td>\n      <td>1.024017</td>\n      <td>0.391090</td>\n      <td>0.988340</td>\n      <td>0.411828</td>\n      <td>0.393585</td>\n      <td>0.461372</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>C</td>\n      <td>B</td>\n      <td>C</td>\n      <td>A</td>\n      <td>E</td>\n      <td>C</td>\n      <td>F</td>\n      <td>...</td>\n      <td>0.313032</td>\n      <td>0.431007</td>\n      <td>0.390992</td>\n      <td>0.408874</td>\n      <td>0.447887</td>\n      <td>0.390253</td>\n      <td>0.648932</td>\n      <td>0.385935</td>\n      <td>0.370401</td>\n      <td>0.900412</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>199995</th>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>C</td>\n      <td>B</td>\n      <td>D</td>\n      <td>A</td>\n      <td>E</td>\n      <td>E</td>\n      <td>I</td>\n      <td>...</td>\n      <td>0.287454</td>\n      <td>0.543800</td>\n      <td>0.682378</td>\n      <td>1.028978</td>\n      <td>1.022741</td>\n      <td>0.683903</td>\n      <td>0.877273</td>\n      <td>0.532410</td>\n      <td>0.605397</td>\n      <td>0.884581</td>\n    </tr>\n    <tr>\n      <th>199996</th>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>C</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>E</td>\n      <td>C</td>\n      <td>F</td>\n      <td>...</td>\n      <td>0.794881</td>\n      <td>0.432778</td>\n      <td>0.389775</td>\n      <td>0.359871</td>\n      <td>0.550013</td>\n      <td>0.492082</td>\n      <td>0.202295</td>\n      <td>0.416875</td>\n      <td>0.406205</td>\n      <td>0.758665</td>\n    </tr>\n    <tr>\n      <th>199997</th>\n      <td>A</td>\n      <td>B</td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>E</td>\n      <td>C</td>\n      <td>I</td>\n      <td>...</td>\n      <td>0.514487</td>\n      <td>0.060997</td>\n      <td>0.171741</td>\n      <td>0.317185</td>\n      <td>0.150340</td>\n      <td>0.122109</td>\n      <td>0.390524</td>\n      <td>0.334026</td>\n      <td>0.378987</td>\n      <td>0.839416</td>\n    </tr>\n    <tr>\n      <th>199998</th>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>C</td>\n      <td>B</td>\n      <td>D</td>\n      <td>A</td>\n      <td>D</td>\n      <td>A</td>\n      <td>F</td>\n      <td>...</td>\n      <td>0.286144</td>\n      <td>1.061710</td>\n      <td>0.819811</td>\n      <td>0.901241</td>\n      <td>0.555339</td>\n      <td>0.844315</td>\n      <td>0.894193</td>\n      <td>0.794102</td>\n      <td>0.844279</td>\n      <td>0.890473</td>\n    </tr>\n    <tr>\n      <th>199999</th>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>C</td>\n      <td>B</td>\n      <td>D</td>\n      <td>A</td>\n      <td>E</td>\n      <td>A</td>\n      <td>J</td>\n      <td>...</td>\n      <td>0.286755</td>\n      <td>1.065725</td>\n      <td>0.687682</td>\n      <td>0.654738</td>\n      <td>0.574575</td>\n      <td>0.617467</td>\n      <td>0.694336</td>\n      <td>0.745698</td>\n      <td>0.568525</td>\n      <td>0.783568</td>\n    </tr>\n  </tbody>\n</table>\n<p>200000 rows Ã— 24 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"%%time\n# One-Hot Encoding + Ordinal Encoding + Tuning\n# pd.cut \n# Model Tuning + drop cat2\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\noe_cols = ['cat9']\nohe_cols = cat_cols\nohe_cols.remove('cat9')\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[oe_cols] = ordinal_encoder.fit_transform(X_train[oe_cols])\n    X_valid[oe_cols] = ordinal_encoder.transform(X_valid[oe_cols])\n    X_test[oe_cols] = ordinal_encoder.transform(X_test[oe_cols]) # Q. The last transform\n    \n    # Preprocessing - One-Hot Encoding\n    ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    X_train_ohe = ohe.fit_transform(X_train[ohe_cols])\n    X_valid_ohe = ohe.transform(X_valid[ohe_cols])\n    X_test_ohe = ohe.transform(X_test[ohe_cols]) # Q. The last transform\n    \n    X_train_ohe = pd.DataFrame(X_train_ohe, columns=[f\"ohe_{i}\" for i in range(X_train_ohe.shape[1])])\n    X_valid_ohe = pd.DataFrame(X_valid_ohe, columns=[f\"ohe_{i}\" for i in range(X_valid_ohe.shape[1])])\n    X_test_ohe = pd.DataFrame(X_test_ohe, columns=[f\"ohe_{i}\" for i in range(X_test_ohe.shape[1])])\n    \n    X_train = pd.concat([X_train.drop(columns=ohe_cols), X_train_ohe], axis=1)\n    X_valid = pd.concat([X_valid.drop(columns=ohe_cols), X_valid_ohe], axis=1)\n    X_test = pd.concat([X_test.drop(columns=ohe_cols), X_test_ohe], axis=1)\n\n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nprint('You need to reset dataframe!')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T22:42:37.746539Z","iopub.execute_input":"2021-08-24T22:42:37.746908Z","iopub.status.idle":"2021-08-24T22:43:05.859597Z","shell.execute_reply.started":"2021-08-24T22:42:37.746864Z","shell.execute_reply":"2021-08-24T22:43:05.857667Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"0 0.71869890821944\n1 0.7184030212315331\n2 0.7204136662080575\n3 0.7203299560598941\n4 0.7191436344980804\n0.719397837243401 0.0008298835686080188\nYou need to reset dataframe!\nCPU times: user 27.7 s, sys: 1.83 s, total: 29.5 s\nWall time: 28.1 s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Model Tuning + drop cat2\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\", \"cat2\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    #model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nprint('You need to reset dataframe!')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:54:06.321382Z","iopub.execute_input":"2021-08-24T21:54:06.321743Z","iopub.status.idle":"2021-08-24T21:54:33.276709Z","shell.execute_reply.started":"2021-08-24T21:54:06.321709Z","shell.execute_reply":"2021-08-24T21:54:33.275794Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"0 0.718609466420021\n1 0.7185071559268716\n2 0.7202768090254914\n3 0.7204231710582014\n4 0.719266186984806\n0.7194165578830783 0.0008067616831622805\nCPU times: user 26.9 s, sys: 676 ms, total: 27.6 s\nWall time: 26.9 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# Model Tuning + drop cat2, cat6\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\", \"cat2\", \"cat6\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    #model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nprint('You need to reset dataframe!')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:55:30.031254Z","iopub.execute_input":"2021-08-24T21:55:30.031587Z","iopub.status.idle":"2021-08-24T21:55:54.609825Z","shell.execute_reply.started":"2021-08-24T21:55:30.031558Z","shell.execute_reply":"2021-08-24T21:55:54.608783Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"0 0.7187555969676352\n1 0.7184220257043639\n2 0.7204441725501761\n3 0.7202437427874456\n4 0.719224667690094\n0.7194180411399429 0.0008003750623549543\nCPU times: user 24.6 s, sys: 675 ms, total: 25.3 s\nWall time: 24.6 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# Tuning + Standardization\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Preprocessing - Standardization\n    scaler = StandardScaler()\n    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n    X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n    X_test[num_cols] = scaler.transform(X_test[num_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    #model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T22:31:05.898610Z","iopub.execute_input":"2021-08-24T22:31:05.898981Z","iopub.status.idle":"2021-08-24T22:31:33.912625Z","shell.execute_reply.started":"2021-08-24T22:31:05.898945Z","shell.execute_reply":"2021-08-24T22:31:33.911724Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"0 0.7188216743231635\n1 0.7185658026731828\n2 0.7204336784139413\n3 0.7202232260837255\n4 0.719103007983702\n0.7194294778955429 0.0007563593779478354\nCPU times: user 28.3 s, sys: 410 ms, total: 28.7 s\nWall time: 28 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# Only Model Tuning\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:50:43.551510Z","iopub.execute_input":"2021-08-24T21:50:43.551835Z","iopub.status.idle":"2021-08-24T21:51:12.086803Z","shell.execute_reply.started":"2021-08-24T21:50:43.551803Z","shell.execute_reply":"2021-08-24T21:51:12.085859Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"0 0.7187526657413477\n1 0.7183753362008053\n2 0.7204385881841233\n3 0.7202580864662346\n4 0.7190828534147123\n0.7193815060014447 0.0008225359761356826\nCPU times: user 28.5 s, sys: 792 ms, total: 29.3 s\nWall time: 28.5 s\n","output_type":"stream"}]},{"cell_type":"code","source":"# Export submission.csv\npreds = np.mean(np.column_stack(final_predictions), axis=1)\npreds = pd.DataFrame({'id': sample_submission.id, 'target': preds})\npreds.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:48:43.959974Z","iopub.execute_input":"2021-08-24T21:48:43.960328Z","iopub.status.idle":"2021-08-24T21:48:44.474981Z","shell.execute_reply.started":"2021-08-24T21:48:43.960296Z","shell.execute_reply":"2021-08-24T21:48:44.474102Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}