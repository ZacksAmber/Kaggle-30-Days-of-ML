{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle 30 Days of Machine Learning - Competition\n\n## Introduction\n\n### Author Info\n\n- Author: [Zacks Shen](https://www.linkedin.com/in/zacks-shen/)\n- Blog: [Zacks.One](https://zacks.one)\n\n---\n\n### Reference\n\n> [A Brief Overview of Outlier Detection Techniques](https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561)\n\n---\n\n### Project Intro\n\nFor this competition, you will be predicting a continuous `target` based on a number of feature columns given in the data. All of the feature columns, `cat0` - `cat9` are categorical, and the feature columns `cont0` - `cont13` are continuous.\n\n**Files**\n- **train.csv** - the training data with the `target` column\n- **test.csv** - the test set; you will be predicting the `target` for each row in this file\n- **sample_submission.csv** - a sample submission file in the correct format","metadata":{}},{"cell_type":"markdown","source":"---\n\n# Dependencies","metadata":{}},{"cell_type":"code","source":"# Statistics\nimport pandas as pd\nimport numpy as np\nimport math as mt\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# Data Preprocessing - Standardization, Encoding, Imputation\nfrom sklearn.preprocessing import StandardScaler # Standardization\nfrom sklearn.preprocessing import Normalizer # Normalization\nfrom sklearn.preprocessing import OneHotEncoder # One-hot Encoding\nfrom sklearn.preprocessing import OrdinalEncoder # Ordinal Encoding\nfrom category_encoders import MEstimateEncoder # Target Encoding\nfrom sklearn.preprocessing import PolynomialFeatures # Create Polynomial Features\nfrom sklearn.impute import SimpleImputer # Imputation\n\n# Exploratory Data Analysis - Feature Engineering\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.decomposition import PCA\n\n# Modeling - ML Pipelines\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\n# Modeling - Algorithms\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\n\n# ML - Evaluation\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\n# ML - Tuning\nimport optuna\n#from sklearn.model_selection import GridSearchCV\n\n# Settings\n# Settings for Seaborn\nsns.set_theme(context='notebook', style='ticks', palette=\"bwr_r\", font_scale=0.7, rc={\"figure.dpi\":240, 'savefig.dpi':240})","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-31T20:00:21.696952Z","iopub.execute_input":"2021-08-31T20:00:21.697588Z","iopub.status.idle":"2021-08-31T20:00:25.701468Z","shell.execute_reply.started":"2021-08-31T20:00:21.697492Z","shell.execute_reply":"2021-08-31T20:00:25.700447Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"---\n\n# Dataset","metadata":{}},{"cell_type":"code","source":"import os\nkaggle_project = '30-days-of-ml'\n# Import dataset from local directory './data' or from Kaggle\ndata_dir = ('./data' if os.path.exists('data') else f'/kaggle/input/{kaggle_project}')\n\n# print all files in data_dir\nfor dirname, _, filenames in os.walk(data_dir):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-30T20:37:17.912429Z","iopub.execute_input":"2021-08-30T20:37:17.91276Z","iopub.status.idle":"2021-08-30T20:37:17.919798Z","shell.execute_reply.started":"2021-08-30T20:37:17.91273Z","shell.execute_reply":"2021-08-30T20:37:17.918796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(f'{data_dir}/train.csv')\ntest_data = pd.read_csv(f'{data_dir}/test.csv')\n\ntrain_data.head()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-30T20:37:20.009582Z","iopub.execute_input":"2021-08-30T20:37:20.00995Z","iopub.status.idle":"2021-08-30T20:37:23.238996Z","shell.execute_reply.started":"2021-08-30T20:37:20.009904Z","shell.execute_reply":"2021-08-30T20:37:23.238127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## Missing Values\n\nSince the train dataset does not have any missing value, there is not necessary for data imputation.","metadata":{}},{"cell_type":"code","source":"# print number missing values from train_data and test_data\ntrain_data.isna().sum().sum(), test_data.isna().sum().sum()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-30T03:38:46.850077Z","iopub.execute_input":"2021-08-30T03:38:46.850315Z","iopub.status.idle":"2021-08-30T03:38:47.287671Z","shell.execute_reply.started":"2021-08-30T03:38:46.850291Z","shell.execute_reply":"2021-08-30T03:38:47.286798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## Target Distribution\n\n99.82% target data is located in 6.5 to 10.5.","metadata":{}},{"cell_type":"code","source":"sns.histplot(x=train_data['target'], stat='density')\nplt.title('Distribution of All Targets')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:38:47.289221Z","iopub.execute_input":"2021-08-30T03:38:47.289464Z","iopub.status.idle":"2021-08-30T03:38:48.206941Z","shell.execute_reply.started":"2021-08-30T03:38:47.289441Z","shell.execute_reply":"2021-08-30T03:38:48.205922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Main target range\nlen(train_data[(6.5 <= train_data.target) & (train_data.target < 10.5)]) / len(train_data)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-29T07:36:03.331387Z","iopub.execute_input":"2021-08-29T07:36:03.331731Z","iopub.status.idle":"2021-08-29T07:36:03.392336Z","shell.execute_reply.started":"2021-08-29T07:36:03.331697Z","shell.execute_reply":"2021-08-29T07:36:03.391443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## Numerical Data\n\nSince all of the numerical data are in the same scale, there is no need for scaling.\n\n\n**Conclusions**\n- The numerical features are all non-parametric. Therefore, we should implement non-parametric outliers dectection.\n- The distributions of numerical features from train and test dataset are similar.","metadata":{}},{"cell_type":"code","source":"# Define num_cols storing the columns names of numerical features\nnum_cols = list(train_data.select_dtypes(include=['int', 'float']).columns)\nnum_cols.remove('id')\nnum_cols.remove('target')\nprint(num_cols)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T20:37:45.699815Z","iopub.execute_input":"2021-08-30T20:37:45.70015Z","iopub.status.idle":"2021-08-30T20:37:45.719801Z","shell.execute_reply.started":"2021-08-30T20:37:45.700119Z","shell.execute_reply":"2021-08-30T20:37:45.718943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[num_cols].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:36:24.154833Z","iopub.execute_input":"2021-08-29T07:36:24.15517Z","iopub.status.idle":"2021-08-29T07:36:24.339449Z","shell.execute_reply.started":"2021-08-29T07:36:24.155139Z","shell.execute_reply":"2021-08-29T07:36:24.338327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[num_cols].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:36:24.359358Z","iopub.execute_input":"2021-08-29T07:36:24.359624Z","iopub.status.idle":"2021-08-29T07:36:24.497671Z","shell.execute_reply.started":"2021-08-29T07:36:24.359579Z","shell.execute_reply":"2021-08-29T07:36:24.496707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plots(plot, features, train=None, test=None, target=None, ncols=4, figsize=(20, 16), **params):\n    \"\"\"Recieves an Seaborn method and one or two Pandas DataFrames, then returns subplots.\n    \n    Args:\n        plot: A Seaborn instance.\n        features: A set of Pandas DataFrame columns.\n        train: Train dataset.\n        test: Test dataset.\n        target: The target, default is None.\n        ncols: Number of figures in a row, default is 4.\n        figsize: Figure Size, default is (20, 10).\n    \n    **kwargs:\n        params: The parameters for Seaborn instance.\n    \n    Returns:\n        The matplotlib.pyplot.subplots instance.\n    \"\"\"\n    # Set row, col, and figure id\n    nfigs = len(features)\n    nrows = mt.ceil(nfigs/ncols)  # number of rows\n    fig_id = 1  # initialize plot counter\n\n    fig = plt.figure(figsize=figsize)\n    for feature in features:\n        plt.subplot(nrows, ncols, fig_id)\n        if isinstance(train, pd.DataFrame):\n            plot(data=train, x=feature, y=target, label='train', \n                 **params)\n        if isinstance(test, pd.DataFrame):  \n            plot(data=test, x=feature, label='test', # Test dataset doesn't have target\n                 **params)\n            plt.legend() # Force legend\n        fig_id += 1\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:25:49.210144Z","iopub.execute_input":"2021-08-30T19:25:49.210591Z","iopub.status.idle":"2021-08-30T19:25:49.218681Z","shell.execute_reply.started":"2021-08-30T19:25:49.21056Z","shell.execute_reply":"2021-08-30T19:25:49.217532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Density plots of train and test\nplots(plot=sns.kdeplot, train=train_data, test=test_data, features=num_cols)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:25:51.99504Z","iopub.execute_input":"2021-08-30T19:25:51.995392Z","iopub.status.idle":"2021-08-30T19:26:26.034865Z","shell.execute_reply.started":"2021-08-30T19:25:51.995361Z","shell.execute_reply":"2021-08-30T19:26:26.034105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Scattor plots of train\nplots(plot=sns.scatterplot, train=train_data, features=num_cols, target='target')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:26:26.036239Z","iopub.execute_input":"2021-08-30T19:26:26.036704Z","iopub.status.idle":"2021-08-30T19:27:59.174428Z","shell.execute_reply.started":"2021-08-30T19:26:26.036665Z","shell.execute_reply":"2021-08-30T19:27:59.173639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Correlation with target:\\n\")\npd.DataFrame(train_data[num_cols].corrwith(train_data.target), columns=['Correlation Coefficient']).style.bar(align='mid', color=['#d65f5f', '#5fba7d'])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-30T19:27:59.176024Z","iopub.execute_input":"2021-08-30T19:27:59.176491Z","iopub.status.idle":"2021-08-30T19:27:59.279612Z","shell.execute_reply.started":"2021-08-30T19:27:59.176453Z","shell.execute_reply":"2021-08-30T19:27:59.278642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## Categorical Data\n\nDue to the imbalanced distribution of categorical features, it is worth to implement target encoding.","metadata":{}},{"cell_type":"code","source":"# Define Categorical Columns\ncat_cols = list(train_data.select_dtypes(include=['object']).columns)\nprint(cat_cols)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-30T20:37:50.961966Z","iopub.execute_input":"2021-08-30T20:37:50.962308Z","iopub.status.idle":"2021-08-30T20:37:50.998073Z","shell.execute_reply.started":"2021-08-30T20:37:50.962277Z","shell.execute_reply":"2021-08-30T20:37:50.997091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Countplots of train and test\nplots(plot=sns.countplot, train=train_data, test=test_data, features=cat_cols)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:27:59.281256Z","iopub.execute_input":"2021-08-30T19:27:59.281603Z","iopub.status.idle":"2021-08-30T19:28:05.75703Z","shell.execute_reply.started":"2021-08-30T19:27:59.281567Z","shell.execute_reply":"2021-08-30T19:28:05.756194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Violinplot of train\nplots(plot=sns.violinplot, train=train_data, features=cat_cols, target='target')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:28:05.758248Z","iopub.execute_input":"2021-08-30T19:28:05.758744Z","iopub.status.idle":"2021-08-30T19:28:18.212009Z","shell.execute_reply.started":"2021-08-30T19:28:05.758707Z","shell.execute_reply":"2021-08-30T19:28:18.211236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## Outliers","metadata":{}},{"cell_type":"code","source":"def detect_outliers(data_frame, features):\n    df = data_frame.copy()\n    for feature in features:\n        Q1, Q3 = df[feature].quantile(0.25), df[feature].quantile(0.75)\n        IQR = Q3 - Q1\n        min_value, max_value = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n        filter1, filter2 = min_value <= df[feature], df[feature] <= max_value\n        proportion = 1- len(df[filter1 & filter2]) / len(df)\n        print(f\"{feature}'s outliers proportion: {proportion}\")\n\ndetect_outliers(train_data, num_cols)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T20:37:55.673756Z","iopub.execute_input":"2021-08-30T20:37:55.674118Z","iopub.status.idle":"2021-08-30T20:37:56.467749Z","shell.execute_reply.started":"2021-08-30T20:37:55.674085Z","shell.execute_reply":"2021-08-30T20:37:56.466324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tukey_rule(data_frame, features):\n    \"\"\"Detect outliers for each feature according to Tukey's Rule.\n    \n    Args:\n        data_frame: A Pandas DataFrame instance.\n        features: A set of Pandas DataFrame columns.\n    \n    Returns:\n        A Pandas DataFrame with no outliers.\n    \"\"\"\n    df = data_frame.copy()\n    for feature in features:\n        Q1, Q3 = df[feature].quantile(0.25), df[feature].quantile(0.75)\n        IQR = Q3 - Q1\n        min_value, max_value = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n        filter1, filter2 = min_value <= df[feature], df[feature] <= max_value\n        df = df[filter1 & filter2] # Overwrite df to keep the index same.\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-30T20:37:57.775995Z","iopub.execute_input":"2021-08-30T20:37:57.776346Z","iopub.status.idle":"2021-08-30T20:37:57.784971Z","shell.execute_reply.started":"2021-08-30T20:37:57.776314Z","shell.execute_reply":"2021-08-30T20:37:57.784179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = tukey_rule(train_data[num_cols], num_cols)\ndf_train = df_train.join(train_data[['id'] + cat_cols + ['target']], how='inner') # Join the rest columns\ndf_train = df_train[train_data.columns] # Sort the columns","metadata":{"execution":{"iopub.status.busy":"2021-08-30T20:37:58.079666Z","iopub.execute_input":"2021-08-30T20:37:58.080009Z","iopub.status.idle":"2021-08-30T20:37:58.538979Z","shell.execute_reply.started":"2021-08-30T20:37:58.079975Z","shell.execute_reply":"2021-08-30T20:37:58.538028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.to_csv('df_train.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T20:37:58.774368Z","iopub.execute_input":"2021-08-30T20:37:58.774706Z","iopub.status.idle":"2021-08-30T20:38:06.594555Z","shell.execute_reply.started":"2021-08-30T20:37:58.774676Z","shell.execute_reply":"2021-08-30T20:38:06.593669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# Feature Selection\n\nThe Goals of feature engineering:\n- improve a model's predictive performance\n- reduce computational or data needs\n- improve interpretability of the results\n\nThe possible ways of feature engineering:\n- Creating new features\n- K-means clustering\n- Pinciple Component Analysis\n\nMetric of feature engineering:\n- Mutual Information","metadata":{}},{"cell_type":"markdown","source":"---\n\n## Mutual Information","metadata":{}},{"cell_type":"code","source":"def make_mi_scores(X, y):\n    X = X.copy()\n    # Mutual Information required all data be integers\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize() # factorize() returns code and uniques\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    \n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-30T19:28:35.330773Z","iopub.execute_input":"2021-08-30T19:28:35.331136Z","iopub.status.idle":"2021-08-30T19:28:35.337196Z","shell.execute_reply.started":"2021-08-30T19:28:35.331105Z","shell.execute_reply":"2021-08-30T19:28:35.336318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX = df_train.copy()\nX.drop(columns=['id'], inplace=True)\ny = X.pop(\"target\")\n\n# Review the MI score from all data\nall_mi_scores = pd.DataFrame(make_mi_scores(X, y))\n\nall_mi_scores.style.bar(align='mid', color=['#d65f5f', '#5fba7d'])","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:28:35.774489Z","iopub.execute_input":"2021-08-30T19:28:35.774778Z","iopub.status.idle":"2021-08-30T19:31:08.690722Z","shell.execute_reply.started":"2021-08-30T19:28:35.774749Z","shell.execute_reply":"2021-08-30T19:31:08.689766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### PCA","metadata":{}},{"cell_type":"code","source":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-30T19:31:08.692665Z","iopub.execute_input":"2021-08-30T19:31:08.693025Z","iopub.status.idle":"2021-08-30T19:31:08.702761Z","shell.execute_reply.started":"2021-08-30T19:31:08.692989Z","shell.execute_reply":"2021-08-30T19:31:08.701792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Correlation with target:\\n\")\npd.DataFrame(X.corrwith(y), columns=['Correlation Coefficient']).style.bar(align='mid', color=['#d65f5f', '#5fba7d'])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-30T19:31:08.704564Z","iopub.execute_input":"2021-08-30T19:31:08.704937Z","iopub.status.idle":"2021-08-30T19:31:08.763987Z","shell.execute_reply.started":"2021-08-30T19:31:08.704883Z","shell.execute_reply":"2021-08-30T19:31:08.763093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_data.copy()\ny = X.pop(\"target\")\n\n# `apply_pca`, defined above, reproduces the code from the tutorial\npca, X_pca, loadings = apply_pca(X[num_cols])\nloadings","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-30T19:31:08.765644Z","iopub.execute_input":"2021-08-30T19:31:08.766001Z","iopub.status.idle":"2021-08-30T19:31:09.011669Z","shell.execute_reply.started":"2021-08-30T19:31:08.765966Z","shell.execute_reply":"2021-08-30T19:31:09.010861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_variance(pca);","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-30T19:31:09.012965Z","iopub.execute_input":"2021-08-30T19:31:09.013322Z","iopub.status.idle":"2021-08-30T19:31:09.286545Z","shell.execute_reply.started":"2021-08-30T19:31:09.013286Z","shell.execute_reply":"2021-08-30T19:31:09.285824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npca_mi_scores = pd.DataFrame(make_mi_scores(X_pca, y))\n#pca_mi_scores = pd.DataFrame(mi_scores).style.bar(align='mid', color=['#d65f5f', '#5fba7d'])\n\npca_mi_scores.style.bar(align='mid', color=['#d65f5f', '#5fba7d'])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-30T19:31:09.287756Z","iopub.execute_input":"2021-08-30T19:31:09.288105Z","iopub.status.idle":"2021-08-30T19:33:23.649516Z","shell.execute_reply.started":"2021-08-30T19:31:09.288068Z","shell.execute_reply":"2021-08-30T19:33:23.648757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_pca(model, X):\n    # Test PCA numberical data first\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                    random_state=0)\n\n    # Fit the model\n    model.fit(X_train, y_train)\n\n    # Prediction\n    pred = model.predict(X_valid)\n\n    # Calculate RMSE\n    return mt.sqrt(mean_squared_error(y_valid, pred))","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model hyperparameters\nxgb_params = {'n_estimators': 1000,\n              'learning_rate': 0.1,\n              #'subsample': 0.8,\n              'colsample_bytree': 0.3,\n              'max_depth': 3,\n              'booster': 'gbtree', \n              #'reg_lambda': 45.1,\n              #'reg_alpha': 34.9,\n              'random_state': 0,\n              'n_jobs': 4}\n\nmodel = XGBRegressor(**xgb_params, tree_method = 'gpu_hist')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nX = train_data.copy()\ny = X.pop(\"target\")\n\n# numerical data only\n# 0.7276467952234649\n# 0.7228196639975967\n#test_pca(model, X[num_cols])\n\n# pca only\n# 0.7456365639105215\n# 0.7409662927889535\n#test_pca(model, X_pca)\n\n# pac (no 0 pc)\n# 0.744166905080158\n# 0.7413420081098745\n#test_pca(model, X_pca[pca_mi_scores.iloc[0:12, :].index])\n\n# numerical data + pca\n# 0.727961200766366\n# 0.7235085989402649\ntest_pca(model, X[num_cols].join(X_pca))\n\n# numerical data + pca (no 0 pc)\n# 0.7281481337015027\n# 0.7234893735995077\n# test_pca(model, X[num_cols].join(X_pca[pca_mi_scores.iloc[0:12, :].index]))","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## Data Engineering","metadata":{}},{"cell_type":"markdown","source":"---\n\n### Data Pipline","metadata":{}},{"cell_type":"code","source":"X = df_train.copy()\nX.drop(columns=['id'], inplace=True)\ny = X.pop(\"target\")\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncat_cols_nunique = X[cat_cols].nunique()\nonehot_cols = list(cat_cols_nunique[cat_cols_nunique < 10].keys())\nordinal_cols = list(cat_cols_nunique[cat_cols_nunique >= 10].keys())\n\n# Keep selected columns only\n# my_cols = list(num_cols) + list(onehot_cols)\n\nassert len(num_cols)  + len(onehot_cols) + len(ordinal_cols) == X.shape[1], 'feature selection error'","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-29T17:55:54.51464Z","iopub.execute_input":"2021-08-29T17:55:54.514957Z","iopub.status.idle":"2021-08-29T17:55:54.990264Z","shell.execute_reply.started":"2021-08-29T17:55:54.514929Z","shell.execute_reply":"2021-08-29T17:55:54.989394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_pipelie(num_cols, onehot_cols, ordinal_cols):    \n    # Preprocessing for numerical data\n    numerical_transformer = SimpleImputer(strategy='median')\n\n    # Preprocessing of One Hot Encoding\n    onehot_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\n    # Preprocessing of Ordnial Encoding\n    ordinal_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('ordinal', OrdinalEncoder())\n    ])\n\n    # Bundle preprocessing for numerical and categorical data\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numerical_transformer, num_cols),\n            ('onehot', onehot_transformer, onehot_cols),\n            ('ordinal', ordinal_transformer, ordinal_cols)\n        ])\n\n    '''\n    # Bundle preprocessing and modeling code in a pipeline\n    clf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)\n                         ])\n\n    # Preprocessing of training data, fit model \n    clf.fit(X_train, y_train)\n\n    # Preprocessing of validation data, get predictions\n    preds = clf.predict(X_valid)\n    '''\n    return preprocessor","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-08-29T17:55:56.075958Z","iopub.execute_input":"2021-08-29T17:55:56.076314Z","iopub.status.idle":"2021-08-29T17:55:56.082829Z","shell.execute_reply.started":"2021-08-29T17:55:56.076283Z","shell.execute_reply":"2021-08-29T17:55:56.081942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# Modeling\n\n- Algorithms\n    - XGBoost\n    - LightGBM\n\n## Training + Cross Validation","metadata":{}},{"cell_type":"code","source":"def train(model, data_frame, num_cols, onehot_cols, ordinal_cols):\n    \"\"\"Return the average RMSE over 5 CV folds of selected model.\n    \"\"\"\n    y = data_frame[\"target\"]\n    X = data_frame[num_cols + onehot_cols + ordinal_cols]\n\n    # Define pipline\n    preprocessor = data_pipelie(num_cols, onehot_cols, ordinal_cols)\n    reg = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', model)\n    ])\n    # Define cross-validation\n    scores = -1 * cross_val_score(reg, X, y,\n                                  cv=5,\n                                  scoring='neg_root_mean_squared_error')\n\n    i = 0\n    for score in scores:\n        print(f'Fold {i}, RMSE: {score}')\n        i += 1\n    print(f'Average RMSE: {np.mean(scores)}')\n    #return scores","metadata":{"execution":{"iopub.execute_input":"2021-08-28T20:40:14.84057Z","iopub.status.busy":"2021-08-28T20:40:14.840244Z","iopub.status.idle":"2021-08-28T20:40:14.847684Z","shell.execute_reply":"2021-08-28T20:40:14.846778Z","shell.execute_reply.started":"2021-08-28T20:40:14.840541Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model hyperparameters\nxgb_params = {\n    'n_estimators': 1000,\n    'learning_rate': 0.1,\n    #'subsample': 0.8,\n    'colsample_bytree': 0.3,\n    'max_depth': 3,\n    'booster': 'gbtree', \n    #'reg_lambda': 45.1,\n    #'reg_alpha': 34.9,\n    'random_state': 0,\n    'n_jobs': 4\n}\n\nmodel = XGBRegressor(\n    **xgb_params,\n    tree_method='gpu_hist', \n    gpu_id=0, \n    predictor='gpu_predictor'\n)","metadata":{"execution":{"iopub.execute_input":"2021-08-28T20:40:18.054591Z","iopub.status.busy":"2021-08-28T20:40:18.054247Z","iopub.status.idle":"2021-08-28T20:40:18.060893Z","shell.execute_reply":"2021-08-28T20:40:18.059884Z","shell.execute_reply.started":"2021-08-28T20:40:18.054559Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n_one_hot_cols = list(set(onehot_cols) - {'cat2', 'cat4', 'cat6'})\n\ntrain(model, df_train, num_cols, onehot_cols=_one_hot_cols, ordinal_cols=ordinal_cols)","metadata":{"execution":{"iopub.execute_input":"2021-08-28T20:37:18.266491Z","iopub.status.busy":"2021-08-28T20:37:18.266167Z","iopub.status.idle":"2021-08-28T20:38:11.885527Z","shell.execute_reply":"2021-08-28T20:38:11.884782Z","shell.execute_reply.started":"2021-08-28T20:37:18.266462Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n_one_hot_cols = list(set(onehot_cols) - {'cat2', 'cat4'})\n\ntrain(model, df_train, num_cols, onehot_cols=_one_hot_cols, ordinal_cols=ordinal_cols)","metadata":{"execution":{"iopub.execute_input":"2021-08-28T20:38:11.888748Z","iopub.status.busy":"2021-08-28T20:38:11.888498Z","iopub.status.idle":"2021-08-28T20:39:12.315977Z","shell.execute_reply":"2021-08-28T20:39:12.315194Z","shell.execute_reply.started":"2021-08-28T20:38:11.888722Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n_one_hot_cols = list(set(onehot_cols) - {'cat2', 'cat4'})\n_num_cols = list(set(num_cols) - {'cont13'})\n\ntrain(model, df_train, num_cols=_num_cols, onehot_cols=_one_hot_cols, ordinal_cols=ordinal_cols)","metadata":{"execution":{"iopub.execute_input":"2021-08-28T20:46:01.522541Z","iopub.status.busy":"2021-08-28T20:46:01.522218Z","iopub.status.idle":"2021-08-28T20:47:01.824775Z","shell.execute_reply":"2021-08-28T20:47:01.82387Z","shell.execute_reply.started":"2021-08-28T20:46:01.522513Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## Hyperparameters Tuning\n\n### Generate KFold datasets","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(f'df_train.csv')\n\n# Mark the train dataset with kfold = 5\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X=df_train)):\n    # For each loop, we only mark the valid data.\n    # Therefore, after kfold loops, we have 5 different valid parts.\\\n    #print(len(valid_idx))\n    df_train.loc[valid_idx, \"kfold\"] = fold\n\n# Export train dataset with kfold mark\ndf_train.to_csv(\"train_fold.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T20:38:12.039098Z","iopub.execute_input":"2021-08-30T20:38:12.039447Z","iopub.status.idle":"2021-08-30T20:38:20.825922Z","shell.execute_reply.started":"2021-08-30T20:38:12.039416Z","shell.execute_reply":"2021-08-30T20:38:20.825103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### Model tuning: Ordinal Encoding","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    \"\"\"Modeling tuning.\n    \"\"\"\n    # Select fold == 0 as valid dataset, others as train dataset.\n    fold = 0\n    #df_train = pd.read_csv('train_fold.csv')\n    df_train = pd.read_csv('../input/30days-folds/train_folds.csv')\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n    \n    # Define dataset\n    num_cols = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n    onehot_cols = ['cat0', 'cat1', 'cat3', 'cat5', 'cat6', 'cat7', 'cat8'] # remove 'cat2', 'cat4' due to the low MI scores\n    ordinal_cols = ['cat9']\n    cat_cols = onehot_cols + ordinal_cols\n    \n    y_train = X_train.pop('target')\n    X_train = X_train[num_cols + onehot_cols + ordinal_cols]\n    y_valid = X_valid.pop('target')\n    X_valid = X_valid[num_cols + onehot_cols + ordinal_cols]\n    \n    # Preprocessing - Ordinal Encoding\n    # For this dataset, One-hot encoding cause low accuracy and low performance.\n    # Therefore, I implemented Ordinal Encoding for all categorical features.\n    oe = OrdinalEncoder()\n    X_train[cat_cols] = oe.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = oe.transform(X_valid[cat_cols])\n    \n    # Hyperparameters for XGBoost\n    \"\"\"\n    xgb_params = {\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-3, 10.0),\n        'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n        'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n        'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 10000),\n        'max_depth': trial.suggest_int(\"max_depth\", 3, 10),\n        'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)\n    }\n    \"\"\"\n    xgb_params = {\n        'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n        'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n        'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 10000),\n        'max_depth': trial.suggest_int(\"max_depth\", 2, 10),\n        'random_state': trial.suggest_categorical('random_state', [1, 42, 2021]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)\n    }\n    \n    model = XGBRegressor(\n            tree_method='gpu_hist',\n            gpu_id=0,\n            predictor='gpu_predictor',\n            **xgb_params)\n    \n    \"\"\"\n    # Hyperparameters for LightGBM\n    lgb_params = {\n        'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n        'num_iterations': trial.suggest_int('num_iterations', 100, 10000),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n        'max_depth': trial.suggest_int('max_depth', 1, 7),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 100),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 100, 2000),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.01, 0.99),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.01, 0.99),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n    \n    model = LGBMRegressor(\n                device='gpu',\n                gpu_platform_id=0,\n                gpu_device_id=0,\n                n_jobs=-1,\n                metric='rmse',\n                **lgb_params\n    )\n    \"\"\"\n    \n    model.fit(\n        X_train, y_train, \n        early_stopping_rounds=300,\n        eval_set=[(X_valid, y_valid)],\n        verbose=5000\n    )\n    \n    valid_preds = model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n    return rmse","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:03:07.188504Z","iopub.execute_input":"2021-08-31T07:03:07.188913Z","iopub.status.idle":"2021-08-31T07:03:07.206706Z","shell.execute_reply.started":"2021-08-31T07:03:07.188877Z","shell.execute_reply":"2021-08-31T07:03:07.205678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#study = optuna.create_study(direction='minimize')\n#study.optimize(objective, n_trials=200) # set n_trials","metadata":{"execution":{"iopub.status.busy":"2021-08-31T08:10:08.89823Z","iopub.execute_input":"2021-08-31T08:10:08.898581Z","iopub.status.idle":"2021-08-31T08:10:08.905279Z","shell.execute_reply.started":"2021-08-31T08:10:08.898552Z","shell.execute_reply":"2021-08-31T08:10:08.904164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### Modeling tuning: Ordinal Encoding + One-hot Encoding","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    \"\"\"Modeling tuning.\n    \"\"\"\n    # Select fold == 0 as valid dataset, others as train dataset.\n    fold = 0\n    #df_train = pd.read_csv('train_fold.csv')\n    df_train = pd.read_csv('../input/30days-folds/train_folds.csv')\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n    \n    # Define dataset\n    num_cols = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n    onehot_cols = ['cat0', 'cat1', 'cat3', 'cat5', 'cat6', 'cat7', 'cat8'] # remove 'cat2', 'cat4' due to the low MI scores\n    ordinal_cols = ['cat9']\n    cat_cols = onehot_cols + ordinal_cols\n    \n    y_train = X_train.pop('target')\n    X_train = X_train[num_cols + onehot_cols + ordinal_cols]\n    y_valid = X_valid.pop('target')\n    X_valid = X_valid[num_cols + onehot_cols + ordinal_cols]\n    \n    # Preprocessing - Ordinal Encoding\n    oe = OrdinalEncoder()\n    X_train[ordinal_cols] = oe.fit_transform(X_train[ordinal_cols])\n    X_valid[ordinal_cols] = oe.transform(X_valid[ordinal_cols])\n    \n    # Preprocessing - One-hot Encoding\n    ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    X_train_ohe = ohe.fit_transform(X_train[onehot_cols])\n    X_valid_ohe = ohe.transform(X_valid[onehot_cols])\n    #X_test_ohe = ohe.transform(X_test[onehot_cols]) # Q. The last transform\n    \n    X_train_ohe = pd.DataFrame(X_train_ohe, columns=[f\"ohe_{i}\" for i in range(X_train_ohe.shape[1])])\n    X_valid_ohe = pd.DataFrame(X_valid_ohe, columns=[f\"ohe_{i}\" for i in range(X_valid_ohe.shape[1])])\n    #X_test_ohe = pd.DataFrame(X_test_ohe, columns=[f\"ohe_{i}\" for i in range(X_test_ohe.shape[1])])\n    \n    X_train = pd.concat([X_train.drop(columns=onehot_cols), X_train_ohe], axis=1)\n    X_valid = pd.concat([X_valid.drop(columns=onehot_cols), X_valid_ohe], axis=1)\n    #X_test = pd.concat([X_test.drop(columns=onehot_cols), X_test_ohe], axis=1)\n\n    # Hyperparameters for XGBoost\n    xgb_params = {\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-3, 10.0),\n        'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n        'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n        'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 10000),\n        'max_depth': trial.suggest_int(\"max_depth\", 3, 10),\n        'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)\n    }\n    \n    model = XGBRegressor(\n            tree_method='gpu_hist',\n            gpu_id=0,\n            predictor='gpu_predictor',\n            **xgb_params)\n    \n    \"\"\"\n    # Hyperparameters for LightGBM\n    lgb_params = {\n        'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n        'num_iterations': trial.suggest_int('num_iterations', 100, 10000),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n        'max_depth': trial.suggest_int('max_depth', 1, 7),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 100),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 100, 2000),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.01, 0.99),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.01, 0.99),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n    \n    model = LGBMRegressor(\n                device='gpu',\n                gpu_platform_id=0,\n                gpu_device_id=0,\n                n_jobs=-1,\n                metric='rmse',\n                **lgb_params\n    )\n    \"\"\"\n    \n    model.fit(\n        X_train, y_train, \n        early_stopping_rounds=300,\n        eval_set=[(X_valid, y_valid)],\n        verbose=5000\n    )\n\n    valid_preds = model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n    return rmse","metadata":{"execution":{"iopub.status.busy":"2021-08-31T00:37:52.666714Z","iopub.execute_input":"2021-08-31T00:37:52.667286Z","iopub.status.idle":"2021-08-31T00:37:52.69364Z","shell.execute_reply.started":"2021-08-31T00:37:52.667245Z","shell.execute_reply":"2021-08-31T00:37:52.692867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#study = optuna.create_study(direction='minimize')\n#study.optimize(objective, n_trials=200) # set n_trials","metadata":{"execution":{"iopub.status.busy":"2021-08-31T05:26:50.840965Z","iopub.execute_input":"2021-08-31T05:26:50.841314Z","iopub.status.idle":"2021-08-31T05:26:50.846245Z","shell.execute_reply.started":"2021-08-31T05:26:50.841282Z","shell.execute_reply":"2021-08-31T05:26:50.845387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### Model tuning: Ordinal Encoding, Standardization","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    \"\"\"Modeling tuning with Standardization.\n    \"\"\"\n    # Select fold == 0 as valid dataset, others as train dataset.\n    fold = 0\n    #df_train = pd.read_csv('train_fold.csv')\n    df_train = pd.read_csv('../input/30days-folds/train_folds.csv')\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n    \n    # Define dataset\n    num_cols = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n    onehot_cols = ['cat0', 'cat1', 'cat3', 'cat5', 'cat6', 'cat7', 'cat8'] # remove 'cat2', 'cat4' due to the low MI scores\n    ordinal_cols = ['cat9']\n    cat_cols = onehot_cols + ordinal_cols\n    \n    y_train = X_train.pop('target')\n    X_train = X_train[num_cols + onehot_cols + ordinal_cols]\n    y_valid = X_valid.pop('target')\n    X_valid = X_valid[num_cols + onehot_cols + ordinal_cols]\n    \n    # Preprocessing - Ordinal Encoding\n    # For this dataset, One-hot encoding cause low accuracy and low performance.\n    # Therefore, I implemented Ordinal Encoding for all categorical features.\n    oe = OrdinalEncoder()\n    X_train[cat_cols] = oe.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = oe.transform(X_valid[cat_cols])\n    \n    # Preprocessing - Standardization\n    scaler = StandardScaler()\n    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n    X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n\n    # Hyperparameters for XGBoost\n    xgb_params = {\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-3, 10.0),\n        'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n        'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n        'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 10000),\n        'max_depth': trial.suggest_int(\"max_depth\", 3, 10),\n        'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)\n    }\n    \n    model = XGBRegressor(\n            tree_method='gpu_hist',\n            gpu_id=0,\n            predictor='gpu_predictor',\n            **xgb_params)\n    \n    \"\"\"\n    # Hyperparameters for LightGBM\n    lgb_params = {\n        'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n        'num_iterations': trial.suggest_int('num_iterations', 100, 10000),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n        'max_depth': trial.suggest_int('max_depth', 1, 7),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 100),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 100, 2000),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.01, 0.99),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.01, 0.99),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n    \n    model = LGBMRegressor(\n                device='gpu',\n                gpu_platform_id=0,\n                gpu_device_id=0,\n                n_jobs=-1,\n                metric='rmse',\n                **lgb_params\n    )\n    \"\"\"\n    \n    model.fit(\n        X_train, y_train, \n        early_stopping_rounds=300,\n        eval_set=[(X_valid, y_valid)],\n        verbose=5000\n    )\n    \n    valid_preds = model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n    return rmse","metadata":{"execution":{"iopub.status.busy":"2021-08-31T02:10:40.464661Z","iopub.execute_input":"2021-08-31T02:10:40.464943Z","iopub.status.idle":"2021-08-31T02:10:40.479648Z","shell.execute_reply.started":"2021-08-31T02:10:40.464912Z","shell.execute_reply":"2021-08-31T02:10:40.478759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#study = optuna.create_study(direction='minimize')\n#study.optimize(objective, n_trials=200) # set n_trials","metadata":{"execution":{"iopub.status.busy":"2021-08-31T05:26:02.385276Z","iopub.execute_input":"2021-08-31T05:26:02.38562Z","iopub.status.idle":"2021-08-31T05:26:02.390648Z","shell.execute_reply.started":"2021-08-31T05:26:02.38559Z","shell.execute_reply":"2021-08-31T05:26:02.389528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### Model tuning: Ordinal Encoding, Log Transformation","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    \"\"\"Modeling tuning with Log transformation.\n    \"\"\"\n    # Select fold == 0 as valid dataset, others as train dataset.\n    fold = 0\n    #df_train = pd.read_csv('train_fold.csv')\n    df_train = pd.read_csv('../input/30days-folds/train_folds.csv')\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n    \n    # Define dataset\n    num_cols = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n    onehot_cols = ['cat0', 'cat1', 'cat3', 'cat5', 'cat6', 'cat7', 'cat8'] # remove 'cat2', 'cat4' due to the low MI scores\n    ordinal_cols = ['cat9']\n    cat_cols = onehot_cols + ordinal_cols\n    \n    y_train = X_train.pop('target')\n    X_train = X_train[num_cols + onehot_cols + ordinal_cols]\n    y_valid = X_valid.pop('target')\n    X_valid = X_valid[num_cols + onehot_cols + ordinal_cols]\n    \n    # Preprocessing - Ordinal Encoding\n    # For this dataset, One-hot encoding cause low accuracy and low performance.\n    # Therefore, I implemented Ordinal Encoding for all categorical features.\n    oe = OrdinalEncoder()\n    X_train[cat_cols] = oe.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = oe.transform(X_valid[cat_cols])\n\n    # Preprocessing - Log transformation\n    for col in num_cols:\n        X_train[col] = np.log1p(X_train[col])\n        X_valid[col] = np.log1p(X_valid[col])\n        #test_data[col] = np.log1p(test_data[col])\n    \n    # Hyperparameters for XGBoost\n    xgb_params = {\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-3, 10.0),\n        'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n        'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n        'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 10000),\n        'max_depth': trial.suggest_int(\"max_depth\", 3, 10),\n        'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)\n    }\n    \n    model = XGBRegressor(\n            tree_method='gpu_hist',\n            gpu_id=0,\n            predictor='gpu_predictor',\n            **xgb_params)\n    \n    \"\"\"\n    # Hyperparameters for LightGBM\n    lgb_params = {\n        'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n        'num_iterations': trial.suggest_int('num_iterations', 100, 10000),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n        'max_depth': trial.suggest_int('max_depth', 1, 7),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 100),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 100, 2000),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.01, 0.99),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.01, 0.99),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n    \n    model = LGBMRegressor(\n                device='gpu',\n                gpu_platform_id=0,\n                gpu_device_id=0,\n                n_jobs=-1,\n                metric='rmse',\n                **lgb_params\n    )\n    \"\"\"\n    \n    model.fit(\n        X_train, y_train, \n        early_stopping_rounds=300,\n        eval_set=[(X_valid, y_valid)],\n        verbose=5000\n    )\n    \n    valid_preds = model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n    return rmse","metadata":{"execution":{"iopub.status.busy":"2021-08-31T03:20:46.893866Z","iopub.execute_input":"2021-08-31T03:20:46.894248Z","iopub.status.idle":"2021-08-31T03:20:46.908851Z","shell.execute_reply.started":"2021-08-31T03:20:46.89421Z","shell.execute_reply":"2021-08-31T03:20:46.907741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#study = optuna.create_study(direction='minimize')\n#study.optimize(objective, n_trials=200) # set n_trials","metadata":{"execution":{"iopub.status.busy":"2021-08-31T05:20:31.081307Z","iopub.execute_input":"2021-08-31T05:20:31.081652Z","iopub.status.idle":"2021-08-31T05:20:31.089471Z","shell.execute_reply.started":"2021-08-31T05:20:31.081619Z","shell.execute_reply":"2021-08-31T05:20:31.088575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### Model tuning: Target Encoding","metadata":{}},{"cell_type":"code","source":"%%time\n# Find the best m for target encoding.\ndef t_enc_baseline(m_list):\n    \"\"\"Return baseline rmse for each m of target encoding.\n    \"\"\"\n    import warnings\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\n    # Define dataset\n    fold = 0\n    #df_train = pd.read_csv('train_fold.csv')\n    df_train = pd.read_csv('../input/30days-folds/train_folds.csv')\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n    \n    num_cols = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n    onehot_cols = ['cat0', 'cat1', 'cat3', 'cat5', 'cat6', 'cat7', 'cat8'] # remove 'cat2', 'cat4' due to the low MI scores\n    ordinal_cols = ['cat9']\n    cat_cols = onehot_cols + ordinal_cols\n    \n    y_train = X_train.pop('target')\n    X_train = X_train[num_cols + onehot_cols + ordinal_cols]\n    y_valid = X_valid.pop('target')\n    X_valid = X_valid[num_cols + onehot_cols + ordinal_cols]\n    \n    # Define model\n    model = XGBRegressor(tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    \n    scores = {}\n    for m in m_list:\n        # Target Encoding\n        te = MEstimateEncoder(cols=cat_cols, m=m)\n        X_train = te.fit_transform(X_train, y_train)\n        X_valid = te.transform(X_valid)\n\n        # Modeling - Training\n        model.fit(X_train, y_train)\n        valid_preds = model.predict(X_valid)\n        \n        # Modeling - Evaluation\n        rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n        \n        scores[f'm={m}'] = rmse\n    return scores\n\n_scores = t_enc_baseline(range(50))\nprint(f'The m of best rmse: {min(_scores, key=scores.get)}')\nprint(f'The best rmse: {min(_scores.values())}')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T23:40:47.337219Z","iopub.execute_input":"2021-08-30T23:40:47.337559Z","iopub.status.idle":"2021-08-30T23:42:00.691431Z","shell.execute_reply.started":"2021-08-30T23:40:47.33753Z","shell.execute_reply":"2021-08-30T23:42:00.690657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    \"\"\"Modeling tuning with Target encoding.\n    \"\"\"\n    import warnings\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n    \n    # Select fold == 0 as valid dataset, others as train dataset.\n    fold = 0\n    #df_train = pd.read_csv('train_fold.csv')\n    df_train = pd.read_csv('../input/30days-folds/train_folds.csv')\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n    \n    # Define dataset\n    num_cols = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n    onehot_cols = ['cat0', 'cat1', 'cat3', 'cat5', 'cat6', 'cat7', 'cat8'] # remove 'cat2', 'cat4' due to the low MI scores\n    ordinal_cols = ['cat9']\n    cat_cols = onehot_cols + ordinal_cols\n    \n    y_train = X_train.pop('target')\n    X_train = X_train[num_cols + onehot_cols + ordinal_cols]\n    y_valid = X_valid.pop('target')\n    X_valid = X_valid[num_cols + onehot_cols + ordinal_cols]\n    \n    # Preprocessing - Target Encoding\n    te = MEstimateEncoder(cols=cat_cols, m=8) # m is from previous step\n    X_train = te.fit_transform(X_train, y_train)\n    X_valid = te.transform(X_valid)\n    \n    # Hyperparameters for XGBoost\n    xgb_params = {\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-3, 10.0),\n        'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n        'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n        'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 10000),\n        'max_depth': trial.suggest_int(\"max_depth\", 3, 10),\n        'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)\n    }\n    \n    model = XGBRegressor(\n            tree_method='gpu_hist',\n            gpu_id=0,\n            predictor='gpu_predictor',\n            **xgb_params)\n    \n    \"\"\"\n    # Hyperparameters for LightGBM\n    lgb_params = {\n        'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n        'num_iterations': trial.suggest_int('num_iterations', 100, 10000),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n        'max_depth': trial.suggest_int('max_depth', 1, 7),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 100),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 100, 2000),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.01, 0.99),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.01, 0.99),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n    \n    model = LGBMRegressor(\n                device='gpu',\n                gpu_platform_id=0,\n                gpu_device_id=0,\n                n_jobs=-1,\n                metric='rmse',\n                **lgb_params\n    )\n    \"\"\"\n    \n    model.fit(\n        X_train, y_train, \n        early_stopping_rounds=300,\n        eval_set=[(X_valid, y_valid)],\n        verbose=5000\n    )\n    \n    valid_preds = model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n    return rmse","metadata":{"execution":{"iopub.status.busy":"2021-08-31T04:28:09.375217Z","iopub.execute_input":"2021-08-31T04:28:09.375565Z","iopub.status.idle":"2021-08-31T04:28:09.389501Z","shell.execute_reply.started":"2021-08-31T04:28:09.375526Z","shell.execute_reply":"2021-08-31T04:28:09.388458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#study = optuna.create_study(direction='minimize')\n#study.optimize(objective, n_trials=200) # set n_trials","metadata":{"execution":{"iopub.status.busy":"2021-08-31T05:19:39.923842Z","iopub.execute_input":"2021-08-31T05:19:39.924302Z","iopub.status.idle":"2021-08-31T05:19:39.934017Z","shell.execute_reply.started":"2021-08-31T05:19:39.924259Z","shell.execute_reply":"2021-08-31T05:19:39.933137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_params","metadata":{"execution":{"iopub.status.busy":"2021-08-31T05:21:17.888402Z","iopub.execute_input":"2021-08-31T05:21:17.888726Z","iopub.status.idle":"2021-08-31T05:21:17.896209Z","shell.execute_reply.started":"2021-08-31T05:21:17.888695Z","shell.execute_reply":"2021-08-31T05:21:17.895264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## Model Blending","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:17:07.51522Z","iopub.execute_input":"2021-08-29T08:17:07.515539Z","iopub.status.idle":"2021-08-29T08:17:07.654989Z","shell.execute_reply.started":"2021-08-29T08:17:07.515509Z","shell.execute_reply":"2021-08-29T08:17:07.654127Z"}}},{"cell_type":"code","source":"class Modeling:\n    def __init__(self, train_path):\n        import warnings\n        warnings.filterwarnings('ignore')\n\n        # Import datasets\n        self.df_train = pd.read_csv(train_path)\n        self.df_test = pd.read_csv('../input/30-days-of-ml/test.csv')\n        #self.df_test = pd.read_csv('data/test.csv')\n        self.sample_submission = pd.read_csv('../input/30-days-of-ml/sample_submission.csv')\n        \n        # Define features\n        self.num_cols = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n        self.onehot_cols = ['cat0', 'cat1', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8'] # remove 'cat2' due to the low MI scores\n        self.ordinal_cols = ['cat9']\n        self.cat_cols = self.onehot_cols + self.ordinal_cols\n        self.useful_features = self.num_cols + self.cat_cols\n        self.target = 'target'\n    \n    # Preprocessing solution 0: Ordinal Encoding\n    def _ordinal_encoding(self, X_train, X_valid, X_test, params=True):\n        # Preprocessing - Ordinal Encoding\n        oe = OrdinalEncoder()\n        X_train[self.cat_cols] = oe.fit_transform(X_train[self.cat_cols])\n        X_valid[self.cat_cols] = oe.transform(X_valid[self.cat_cols])\n        X_test[self.cat_cols] = oe.transform(X_test[self.cat_cols])\n        \n        \"\"\"no_outliers\n        # 0.7172987346930846\n        # XGBoost params\n        xgb_params = {\n            'alpha': 7.128681031027614,\n            'lambda': 0.40760576474680843,\n            'gamma': 0.08704298132127238,\n            'reg_alpha': 25.377502919374336,\n            'reg_lambda': 0.003401041649454036,\n            'colsample_bytree': 0.1355660282707954,\n            'subsample': 0.6999406375783235,\n            'learning_rate': 0.02338550339980208,\n            'n_estimators': 9263,\n            'max_depth': 6,\n            'random_state': 2021,\n            'min_child_weight': 138\n        }\n\n        # 0.7174088504920006\n        # LightGBM params\n        lgb_params = {\n            'random_state': 0, \n            'num_iterations': 9530, \n            'learning_rate': 0.018509357813869098, \n            'max_depth': 6, \n            'num_leaves': 98, \n            'min_data_in_leaf': 1772, \n            'lambda_l1': 0.0010866230909549698, \n            'lambda_l2': 1.6105154171511057e-05, \n            'feature_fraction': 0.09911317646202211, \n            'bagging_fraction': 0.8840672050147438, \n            'bagging_freq': 6, \n            'min_child_samples': 35\n        }\n        \"\"\"\n\n        \"\"\"Full dataset\"\"\"\n        # 0.7168956185375995\n        # XGBoost params\n        xgb_params = {\n            'alpha': 0.41478790863509063, \n            'lambda': 4.533806139098733, \n            'gamma': 0.006523052455552593, \n            'reg_alpha': 16.714567692323264, \n            'reg_lambda': 6.321580437513598e-06, \n            'colsample_bytree': 0.11544585116842096, \n            'subsample': 0.8448523684136955, \n            'learning_rate': 0.061677285578690844, \n            'n_estimators': 8676, \n            'max_depth': 3, \n            'random_state': 0, \n            'min_child_weight': 268\n        }\n\n        # 0.716864914502225\n        # LightGBM params\n        lgb_params = {\n            'random_state': 42, \n            'num_iterations': 9718, \n            'learning_rate': 0.014607325219450438, \n            'max_depth': 6, \n            'num_leaves': 11, \n            'min_data_in_leaf': 704, \n            'lambda_l1': 1.1906873176743155e-07, \n            'lambda_l2': 1.0479700691744536, \n            'feature_fraction': 0.13449900909384252, \n            'bagging_fraction': 0.6680657144501343, \n            'bagging_freq': 1, \n            'min_child_samples': 66\n        }\n\n        \n        if params == True:\n            return X_train, X_valid, X_test, xgb_params, lgb_params\n        else:\n            return X_train, X_valid, X_test\n    \n    # Preprocessing solution 1: One-hot Encoding + Ordinal Encoding\n    def _onehot_encoding(self, X_train, X_valid, X_test):\n        # Preprocessing - One-hot Encoding\n        ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n        X_train_ohe = ohe.fit_transform(X_train[self.onehot_cols])\n        X_valid_ohe = ohe.transform(X_valid[self.onehot_cols])\n        X_test_ohe = ohe.transform(X_test[self.onehot_cols])\n\n        X_train_ohe = pd.DataFrame(X_train_ohe, columns=[f\"ohe_{i}\" for i in range(X_train_ohe.shape[1])])\n        X_valid_ohe = pd.DataFrame(X_valid_ohe, columns=[f\"ohe_{i}\" for i in range(X_valid_ohe.shape[1])])\n        X_test_ohe = pd.DataFrame(X_test_ohe, columns=[f\"ohe_{i}\" for i in range(X_test_ohe.shape[1])])\n\n        X_train = pd.concat([X_train.drop(columns=self.onehot_cols), X_train_ohe], axis=1)\n        X_valid = pd.concat([X_valid.drop(columns=self.onehot_cols), X_valid_ohe], axis=1)\n        X_test = pd.concat([X_test.drop(columns=self.onehot_cols), X_test_ohe], axis=1)\n        \n        # Preprocessing - Ordinal Encoding\n        oe = OrdinalEncoder()\n        X_train[self.ordinal_cols] = oe.fit_transform(X_train[self.ordinal_cols])\n        X_valid[self.ordinal_cols] = oe.transform(X_valid[self.ordinal_cols])\n        X_test[self.ordinal_cols] = oe.transform(X_test[self.ordinal_cols])\n    \n        \"\"\"No outliers\n        # 0.7174931253475558\n        # XGBoost params\n        xgb_params = {\n            'alpha': 3.046687193123841,\n            'lambda': 0.7302844649944737,\n            'gamma': 0.10108768743909796,\n            'reg_alpha': 14.711350393993625,\n            'reg_lambda': 1.6855306764481926e-07,\n            'colsample_bytree': 0.15006790036326567,\n            'subsample': 0.9761751211889541,\n            'learning_rate': 0.02730958701307226,\n            'n_estimators': 7897,\n            'max_depth': 4,\n            'random_state': 0,\n            'min_child_weight': 203\n        }\n        \n        # 0.7172624587909345\n        # LightGBM params\n        lgb_params = {\n            'random_state': 42, \n            'num_iterations': 6969, \n            'learning_rate': 0.014404708757048168, \n            'max_depth': 7, \n            'num_leaves': 21, \n            'min_data_in_leaf': 1121, \n            'lambda_l1': 4.1636932334315094e-07, \n            'lambda_l2': 1.0975422991510602e-08, \n            'feature_fraction': 0.08082581387850206, \n            'bagging_fraction': 0.6804475225598854, \n            'bagging_freq': 2, \n            'min_child_samples': 32\n        }\n        \"\"\"\n \n        \"\"\"Full dataset\"\"\"\n        # 0.7169803941400036\n        # XGBoost params\n        xgb_params = {\n            'alpha': 0.006431298298825313, \n            'lambda': 1.9946701540008387, \n            'gamma': 0.004468866854966971, \n            'reg_alpha': 1.7419809828857386e-08, \n            'reg_lambda': 0.25665090431195203, \n            'colsample_bytree': 0.10016560933147275, \n            'subsample': 0.770411681261352, \n            'learning_rate': 0.01350994278419047, \n            'n_estimators': 8378, \n            'max_depth': 5, \n            'random_state': 0, \n            'min_child_weight': 237\n        }\n\n        # 0.7168422881316736\n        # LightGBM params\n        lgb_params = {\n            'random_state': 0, \n            'num_iterations': 9121, \n            'learning_rate': 0.03762205881915334, \n            'max_depth': 5, \n            'num_leaves': 12, \n            'min_data_in_leaf': 1331, \n            'lambda_l1': 1.4246377549177525e-08, \n            'lambda_l2': 0.00031572480246719195, \n            'feature_fraction': 0.07760290667911449, \n            'bagging_fraction': 0.9766045388889536, \n            'bagging_freq': 4, \n            'min_child_samples': 50\n        }\n        \n        return X_train, X_valid, X_test, xgb_params, lgb_params\n\n    # Preprocessing solution 2: Ordinal Encoding + Standardization\n    def _standardization(self, X_train, X_valid, X_test):\n        # Preprocessing - Standardization\n        scaler = StandardScaler()\n        X_train[self.num_cols] = scaler.fit_transform(X_train[self.num_cols])\n        X_valid[self.num_cols] = scaler.transform(X_valid[self.num_cols])\n        X_test[self.num_cols] = scaler.transform(X_test[self.num_cols])\n    \n        \"\"\"No outliers\n        # 0.7172152365762312\n        # XGBoost params\n        xgb_params = {\n            'alpha': 0.029925179326119784,\n            'lambda': 0.12530061860157662,\n            'gamma': 0.5415753114227984,\n            'reg_alpha': 14.992919845445886,\n            'reg_lambda': 0.42076728548917974,\n            'colsample_bytree': 0.10022710624560974,\n            'subsample': 0.5596856445758918,\n            'learning_rate': 0.020866717779139694,\n            'n_estimators': 6852,\n            'max_depth': 7,\n            'random_state': 2021,\n            'min_child_weight': 62\n        }\n        \n        # 0.7173410652198884\n        # LightGBM params\n        lgb_params = {\n            'random_state': 0,\n            'num_iterations': 6439,\n            'learning_rate': 0.03625416364918611,\n            'max_depth': 6,\n            'num_leaves': 11,\n            'min_data_in_leaf': 745,\n            'lambda_l1': 4.1932281223524115e-06,\n            'lambda_l2': 0.043343249414638636,\n            'feature_fraction': 0.08623933710228435,\n            'bagging_fraction': 0.7934935001504152,\n            'bagging_freq': 3,\n            'min_child_samples': 23\n        }\n        \"\"\"\n\n        \"\"\"Full dataset\"\"\"\n        # 0.7169754780128185\n        # XGBoost params\n        xgb_params = {\n            'alpha': 0.001954751535110173, \n            'lambda': 0.4889428995375083, \n            'gamma': 0.2639993537540112, \n            'reg_alpha': 4.613139964829376, \n            'reg_lambda': 2.5644939695394116e-06, \n            'colsample_bytree': 0.10081771115519686, \n            'subsample': 0.7241988095847515, \n            'learning_rate': 0.02383200558701416, \n            'n_estimators': 6995, \n            'max_depth': 4, \n            'random_state': 42, \n            'min_child_weight': 198\n        }\n\n        # 0.7168529799564377\n        # LightGBM params\n        lgb_params = {\n            'random_state': 2021, \n            'num_iterations': 7614, \n            'learning_rate': 0.013334143201183738, \n            'max_depth': 7, \n            'num_leaves': 18, \n            'min_data_in_leaf': 1270, \n            'lambda_l1': 6.801685953700497e-05, \n            'lambda_l2': 6.178728858466813e-08, \n            'feature_fraction': 0.13830499588135214, \n            'bagging_fraction': 0.8060814091341896, \n            'bagging_freq': 3, \n            'min_child_samples': 81\n        }\n        \n        return X_train, X_valid, X_test, xgb_params, lgb_params\n\n    # Preprocessing solution 3: Ordinal Encoding + Log transformation\n    def _log_transformation(self, X_train, X_valid, X_test):\n        # Preprocessing - Log transformation\n        for col in self.num_cols:\n            X_train[col] = np.log1p(X_train[col])\n            X_valid[col] = np.log1p(X_valid[col])\n            X_test[col] = np.log1p(X_test[col])\n\n        \"\"\"No outliers\n        # 0.7172539872780895\n        # XGBoost params\n        xgb_params = {\n            'alpha': 0.08862033338686888, \n            'lambda': 0.003553846716302233, \n            'gamma': 0.4097695581309838, \n            'reg_alpha': 17.808150656220917, \n            'reg_lambda': 1.6112661145526217, \n            'colsample_bytree': 0.11935885763757494, \n            'subsample': 0.7326515814471944, \n            'learning_rate': 0.04006687786137418, \n            'n_estimators': 5239, \n            'max_depth': 5, \n            'random_state': 2021, \n            'min_child_weight': 258\n        }\n\n        # 0.7174737448879298\n        # LightGBM params\n        lgb_params = {\n            'random_state': 0,\n            'num_iterations': 7945,\n            'learning_rate': 0.05205269244224801,\n            'max_depth': 6,\n            'num_leaves': 9,\n            'min_data_in_leaf': 1070,\n            'lambda_l1': 1.0744924634974802e-07,\n            'lambda_l2': 1.1250360028635182,\n            'feature_fraction': 0.10421484055936374,\n            'bagging_fraction': 0.916143112009066,\n            'bagging_freq': 6,\n            'min_child_samples': 20\n        }\n        \"\"\"\n\n        \"\"\"Full dataset\"\"\"\n        # 0.7170384818940932\n        # XGBoost params\n        xgb_params = {\n            'alpha': 0.017253367743182032, \n            'lambda': 1.2312523239198236, \n            'gamma': 0.8870836430062957, \n            'reg_alpha': 9.3294011692425e-08, \n            'reg_lambda': 0.080494664534471, \n            'colsample_bytree': 0.12723293204878566, \n            'subsample': 0.5562373818875186, \n            'learning_rate': 0.01759177927013953, \n            'n_estimators': 7480, \n            'max_depth': 5, \n            'random_state': 42, \n            'min_child_weight': 216\n        }\n\n        # 0.716867820750228\n        # LightGBM params\n        lgb_params = {\n            'random_state': 2021, \n            'num_iterations': 9135, \n            'learning_rate': 0.05204126206296579, \n            'max_depth': 3, \n            'num_leaves': 32, \n            'min_data_in_leaf': 1196, \n            'lambda_l1': 0.07110967369867664, \n            'lambda_l2': 9.981527842388462e-08, \n            'feature_fraction': 0.13002087379571273, \n            'bagging_fraction': 0.6510683790039721, \n            'bagging_freq': 2, \n            'min_child_samples': 19\n        }\n        \n        return X_train, X_valid, X_test, xgb_params, lgb_params\n\n    # Preprocessing solution 4: Target Encoding\n    def _target_encoding(self, X_train, X_valid, X_test, y_train):\n        # Preprocessing - Target Encoding\n        te = MEstimateEncoder(cols=self.cat_cols, m=8) # m is from previous step\n        X_train = te.fit_transform(X_train, y_train)\n        X_valid = te.transform(X_valid)\n        X_test = te.transform(X_test)\n    \n        \"\"\"No outliers\n        # 0.7172617296722674\n        # XGBoost params\n        xgb_params = {\n            'alpha': 0.012609024116174448,\n            'lambda': 0.7990281671135536,\n            'gamma': 0.16689280834519887,\n            'reg_alpha': 16.48576968441873,\n            'reg_lambda': 4.83082534682402e-08,\n            'colsample_bytree': 0.1162304168345657,\n            'subsample': 0.9126362948665406,\n            'learning_rate': 0.05528416190414117,\n            'n_estimators': 9670,\n            'max_depth': 5,\n            'random_state': 42,\n            'min_child_weight': 280\n         }\n\n        # 0.7173917173794985\n        # LightGBM params\n        lgb_params = {\n            'random_state': 2021, \n            'num_iterations': 7977, \n            'learning_rate': 0.01618931564625682, \n            'max_depth': 5, \n            'num_leaves': 50, \n            'min_data_in_leaf': 890, \n            'lambda_l1': 0.003233614433753064, \n            'lambda_l2': 2.0001872037801434e-06, \n            'feature_fraction': 0.13638848986185334, \n            'bagging_fraction': 0.7045068716734475, \n            'bagging_freq': 2, \n            'min_child_samples': 79\n        }\n        \"\"\"\n        \n        \"\"\"Full dataset\"\"\"\n        # 0.7169975934181431\n        # XGBoost params\n        xgb_params = {\n            'alpha': 0.1211523885965823,\n            'lambda': 0.00452864739396485,\n            'gamma': 0.03948208038791913,\n            'reg_alpha': 12.908845680463497,\n            'reg_lambda': 3.120894405337636e-07,\n            'colsample_bytree': 0.10449109016850185,\n            'subsample': 0.7633088122674517,\n            'learning_rate': 0.03246721588939738,\n            'n_estimators': 6866,\n            'max_depth': 5,\n            'random_state': 0,\n            'min_child_weight': 131\n         }\n\n        # 0.7169507121946433\n        # LightGBM params\n        lgb_params = {\n            'random_state': 2021, \n            'num_iterations': 5027, \n            'learning_rate': 0.05104410422762626, \n            'max_depth': 3, \n            'num_leaves': 77, \n            'min_data_in_leaf': 440, \n            'lambda_l1': 0.05579764755559036, \n            'lambda_l2': 4.375963929072086e-08, \n            'feature_fraction': 0.14611731889635768, \n            'bagging_fraction': 0.9005654268392156, \n            'bagging_freq': 1, \n            'min_child_samples': 23\n        }\n        \n        return X_train, X_valid, X_test, xgb_params, lgb_params\n    \n    def _xgboost_reg(self, xgb_params):\n        model = XGBRegressor(\n                    tree_method='gpu_hist',\n                    gpu_id=0,\n                    predictor='gpu_predictor',\n                    n_jobs=-1,\n                    **xgb_params\n                )\n        return model\n    \n    def _lightgbm_reg(self, lgb_params):\n        model = LGBMRegressor(\n                    device='gpu',\n                    gpu_platform_id=0,\n                    gpu_device_id=0,\n                    n_jobs=-1,\n                    metric='rmse',\n                    **lgb_params\n                )\n        return model\n    \n    def blending(self, model: str):\n        '''Model blending. Generate 5 predictions according to 5 data preprocessing solutions.\n        \n        Args:\n            model: One of xgboost or lightgbm\n            \n        Returns:\n            None\n        '''\n        assert model in ['xgboost', 'lightgbm'], \"ValueError: model must be one of ['xgboost', 'lightgbm']!\"\n        \n        # Loop preprocessing solutions\n        for preprocessing_solution in range(5):\n            final_valid_predictions = {} # store final predictions of X_valid for each preprocessing_solution\n            final_test_predictions = [] # store final predictions of X_test for each preprocessing_solution\n            scores = [] # store RMSE scores for each preprocessing_solution\n            print(f\"Data Preprocessing Solution: {preprocessing_solution}, Model: {model}\")\n            print(f\"Training ...\")\n            # Loop KFolds\n            for fold in range(5):\n                # Data Preprocessing\n                X_train = self.df_train[self.df_train.kfold != fold].reset_index(drop=True)\n                X_valid = self.df_train[self.df_train.kfold == fold].reset_index(drop=True)\n                X_test = self.df_test.copy()\n                \n                # get X_valid id\n                X_valid_ids = X_valid.id.values.tolist()\n                \n                y_train = X_train.pop(self.target)\n                X_train = X_train[self.useful_features] # not include id, cat2\n                y_valid = X_valid.pop(self.target)\n                X_valid = X_valid[self.useful_features] # not include id, cat2\n                X_test = X_test[self.useful_features]\n                \n                # Ordinal Encoding\n                if preprocessing_solution == 0:\n                    X_train, X_valid, X_test, xgb_params, lgb_params = self._ordinal_encoding(X_train, X_valid, X_test)\n                # One-hot Encoding + Ordinal Encoding\n                elif preprocessing_solution == 1:\n                    X_train, X_valid, X_test, xgb_params, lgb_params = self._onehot_encoding(X_train, X_valid, X_test)\n                # Ordinal Encoding + Standardization\n                elif preprocessing_solution == 2:\n                    X_train, X_valid, X_test = self._ordinal_encoding(X_train, X_valid, X_test, params=False)\n                    X_train, X_valid, X_test, xgb_params, lgb_params = self._standardization(X_train, X_valid, X_test)\n                # Ordinal Encoding + Log Transformation\n                elif preprocessing_solution == 3:\n                    X_train, X_valid, X_test = self._ordinal_encoding(X_train, X_valid, X_test, params=False)\n                    X_train, X_valid, X_test, xgb_params, lgb_params = self._log_transformation(X_train, X_valid, X_test)\n                # Target Encoding\n                elif preprocessing_solution == 4:\n                    X_train, X_valid, X_test, xgb_params, lgb_params = self._target_encoding(X_train, X_valid, X_test, y_train)\n                \n                # Define model\n                if model == 'xgboost':\n                    reg = self._xgboost_reg(xgb_params)\n                elif model == 'lightgbm':\n                    reg = self._lightgbm_reg(lgb_params)\n                \n                # Modeling - Training\n                reg.fit(\n                    X_train, y_train, \n                    early_stopping_rounds=300,\n                    eval_set=[(X_valid, y_valid)],\n                    verbose=False\n                )\n                \n                # Modeling - Evaluation and Inference\n                valid_preds = reg.predict(X_valid)\n                test_preds = reg.predict(X_test)\n                \n                final_valid_predictions.update(dict(zip(X_valid_ids, valid_preds))) # loop 5 times with different valid id\n                final_test_predictions.append(test_preds) # loop 5 times and get the mean predictions for each row later\n\n                rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n                scores.append(rmse)\n                print(f'Data Preprocessing Solution: {preprocessing_solution}, Fold: {fold}, RMSE: {rmse}')\n                \n            # Export results\n            final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\n            final_valid_predictions.columns = [\"id\", f\"{model}_{preprocessing_solution}_pred\"]\n            final_valid_predictions.to_csv(f\"{model}_{preprocessing_solution}_valid_pred.csv\", index=False)\n\n            test_mean_preds = np.mean(np.column_stack(final_test_predictions), axis=1) # get the meam predictions for each row\n            test_mean_preds = pd.DataFrame({'id': self.sample_submission.id, f\"{model}_{preprocessing_solution}_pred\": test_mean_preds})\n            test_mean_preds.to_csv(f\"{model}_{preprocessing_solution}_test_pred.csv\", index=False)\n            print(f'Average RMSE: {np.mean(scores)}, STD of RMSE: {np.std(scores)}')\n            print('-----------------------------------------------------------------')\n    \n    def _stacking_0(self, X_train, y_train):\n        # Linear Regression stacking\n        model = LinearRegression()\n        model.fit(X_train, y_train)\n        return model\n    \n    def _stacking_1(self, X_train, X_valid, y_train, y_valid):\n        # XGBoost stacking\n        '''\n        # 0.7163584561404919\n        xgb_params = {\n            'alpha': 0.13769710880276775,\n            'lambda': 1.6419112442610602,\n            'gamma': 1.9542744201254334,\n            'reg_alpha': 4.040410591598369e-06,\n            'reg_lambda': 0.0005095115877677602,\n            'colsample_bytree': 0.15171196846336138,\n            'subsample': 0.7792590379642004,\n            'learning_rate': 0.02306515169610187,\n            'n_estimators': 1403,\n            'max_depth': 6,\n            'random_state': 42,\n            'min_child_weight': 161\n        }\n        '''\n        xgb_params = {\n            'random_state': 1, \n            'booster': 'gbtree',\n            'n_estimators': 7000,\n            'learning_rate': 0.03,\n            'max_depth': 2\n        }\n\n        model = XGBRegressor(\n            #tree_method='gpu_hist',\n            #gpu_id=0,\n            #predictor='gpu_predictor',\n            n_jobs=-1,\n            **xgb_params\n        )\n        \n        model.fit(\n            X_train, y_train, \n            early_stopping_rounds=300,\n            eval_set=[(X_valid, y_valid)],\n            verbose=False\n        )\n        \n        return model\n\n    def _stacking_2(self, X_train, X_valid, y_train, y_valid):\n       # LightGBM stacking\n        # 0.7163310756021445\n        lgb_params = {\n            'random_state': 2021,\n            'num_iterations': 4817,\n            'learning_rate': 0.07421712914298478,\n            'max_depth': 4,\n            'num_leaves': 37,\n            'min_data_in_leaf': 1306,\n            'lambda_l1': 0.01948754316174006,\n            'lambda_l2': 0.8125852153169449,\n            'feature_fraction': 0.03432830339337113,\n            'bagging_fraction': 0.8110509749600237,\n            'bagging_freq': 2,\n            'min_child_samples': 22\n        }\n        \n        model = LGBMRegressor(\n                    #device='gpu',\n                    #gpu_platform_id=0,\n                    #gpu_device_id=0,\n                    n_jobs=-1,\n                    metric='rmse',\n                    **lgb_params\n                )\n\n        model.fit(\n            X_train, y_train, \n            early_stopping_rounds=300,\n            eval_set=[(X_valid, y_valid)],\n            verbose=False\n        )\n        \n        return model \n                \n    def stacking(self, data_path: str):\n        '''Model stacking\n        \n        Args:\n            data_path: The path of directory that stores all of the datasets after model blending.\n            \n        Returns:\n            None\n        '''\n        # Define featurs\n        xgb_features = [f\"xgboost_{i}_pred\" for i in range(5)]\n        lgb_features = [f\"lightgbm_{i}_pred\" for i in range(5)]\n        useful_features = xgb_features + lgb_features\n        \n        # Import datasets\n        df_train = pd.read_csv(f'{data_path}/train_folds.csv')\n        df_test = pd.read_csv('../input/30-days-of-ml/test.csv')\n        #data_path = '../input/kaggle30daysofml/kaggle-30days-of-ml/full_dataset'\n        \n        # Join datasets on id\n        for i in range(5):\n            _df_valid = pd.read_csv(f'{data_path}/xgboost_{i}_valid_pred.csv')\n            df_train = df_train.merge(_df_valid, on='id', how='left')\n            _df_test = pd.read_csv(f'{data_path}/xgboost_{i}_test_pred.csv')\n            df_test = df_test.merge(_df_test, on='id', how='left')\n\n        for i in range(5):\n            _df_valid = pd.read_csv(f'{data_path}/lightgbm_{i}_valid_pred.csv')\n            df_train = df_train.merge(_df_valid, on='id', how='left')\n            _df_test = pd.read_csv(f'{data_path}/lightgbm_{i}_test_pred.csv')\n            df_test = df_test.merge(_df_test, on='id', how='left')\n\n        n_stackings = 3\n        # Loop stackings\n        for stacking in range(n_stackings):\n            final_valid_predictions = {} # store final predictions of X_valid for each stacking\n            final_test_predictions = [] # store final predictions of X_test for each stacking\n            scores = [] # store RMSE scores for each preprocessing_solution\n            print(f\"Model Stacking: {stacking}\")\n            print(f\"Training ...\")\n            # Loop KFolds\n            for fold in range(5):\n                X_train =  df_train[df_train.kfold != fold].reset_index(drop=True)\n                X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n                X_test = df_test.copy()\n                \n                # get X_valid id\n                X_valid_ids = X_valid.id.values.tolist()\n\n                y_train = X_train.target\n                y_valid = X_valid.target\n\n                X_train = X_train[useful_features]\n                X_valid = X_valid[useful_features]\n                X_test = X_test[useful_features]\n\n                # Modeling - Training\n                if stacking == 0:\n                    model = self._stacking_0(X_train, y_train)\n                elif stacking == 1:\n                    model = self._stacking_1(X_train, X_valid, y_train, y_valid)\n                elif stacking == 2:\n                    model = self._stacking_2(X_train, X_valid, y_train, y_valid)\n\n                # Modeling - Evaluation and Inference\n                valid_preds = model.predict(X_valid)\n                test_preds = model.predict(X_test)\n                \n                final_valid_predictions.update(dict(zip(X_valid_ids, valid_preds)))\n                final_test_predictions.append(test_preds)\n\n                rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n                scores.append(rmse)\n                print(f'Model Stacking: {stacking}, Fold: {fold}, RMSE: {rmse}')\n                \n            # Export results\n            final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\n            final_valid_predictions.columns = [\"id\", f\"stacking_pred_{stacking}\"]\n            final_valid_predictions.to_csv(f\"stacking_{stacking}_valid_pred.csv\", index=False)\n\n            test_mean_preds = np.mean(np.column_stack(final_test_predictions), axis=1) # get the meam predictions for each row\n            test_mean_preds = pd.DataFrame({'id': self.sample_submission.id, f\"stacking_pred_{stacking}\": test_mean_preds})\n            test_mean_preds.to_csv(f\"stacking_{stacking}_test_pred.csv\", index=False)\n            print(f'Average RMSE: {np.mean(scores)}, STD of RMSE: {np.std(scores)}')\n            print('-----------------------------------------------------------------')\n            \n    def predict(self, data_path: str):\n        '''Predict and generate submission.csv\n            \n        Args:\n            data_path: The path of the train dataset.\n        Returns:\n            None\n        '''\n        n_stackings = 3\n        # Define featurs\n        useful_features = [f\"stacking_pred_{i}\" for i in range(n_stackings)]\n        \n        # Import datasets\n        df_train = pd.read_csv(f'{data_path}/train_folds.csv')\n        df_test = pd.read_csv('../input/30-days-of-ml/test.csv')\n        \n        # Join datasets on id\n        for i in range(n_stackings):\n            _df_valid = pd.read_csv(f'stacking_{i}_valid_pred.csv')\n            df_train = df_train.merge(_df_valid, on='id', how='left')\n            _df_test = pd.read_csv(f'stacking_{i}_test_pred.csv')\n            df_test = df_test.merge(_df_test, on='id', how='left')\n\n        # Modeling\n        final_predictions = []\n        scores = []\n        for fold in range(5):\n            X_train =  df_train[df_train.kfold != fold].reset_index(drop=True)\n            X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n            X_test = df_test.copy()\n\n            y_train = X_train.target\n            y_valid = X_valid.target\n\n            X_train = X_train[useful_features]\n            X_valid = X_valid[useful_features]\n            X_test = X_test[useful_features]\n\n            # Modeling - Training\n            model = LinearRegression()\n            model.fit(X_train, y_train)\n\n            # Modeling - Evaluation and Inference\n            valid_preds = model.predict(X_valid)\n            test_preds = model.predict(X_test)\n            final_predictions.append(test_preds)\n            rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n            print(f'Fold: {fold}, RMSE: {rmse}')\n            scores.append(rmse)\n\n        # Generate submission.csv\n        preds = np.mean(np.column_stack(final_predictions), axis=1)\n        preds = pd.DataFrame({'id': self.sample_submission.id, 'target': preds})\n        preds.to_csv('submission.csv', index=False)\n        \n        print('Generate submission.csv succeed!')\n        print(f'Average RMSE: {np.mean(scores)}, STD of RMSE: {np.std(scores)}')\n        ","metadata":{"execution":{"iopub.status.busy":"2021-08-31T10:54:08.722742Z","iopub.execute_input":"2021-08-31T10:54:08.723423Z","iopub.status.idle":"2021-08-31T10:54:08.815647Z","shell.execute_reply.started":"2021-08-31T10:54:08.723375Z","shell.execute_reply":"2021-08-31T10:54:08.814011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modeling = Modeling(train_path='../input/30days-folds/train_folds.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T10:54:10.338569Z","iopub.execute_input":"2021-08-31T10:54:10.339003Z","iopub.status.idle":"2021-08-31T10:54:13.237563Z","shell.execute_reply.started":"2021-08-31T10:54:10.338959Z","shell.execute_reply":"2021-08-31T10:54:13.236532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# modeling.blending(model='xgboost')\n# modeling.blending(model='lightgbm')\n# modeling.stacking(data_path='../input/kaggle30/no_cat2')\nmodeling.stacking(data_path='../input/kaggle30/full_dataset')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T10:54:13.239469Z","iopub.execute_input":"2021-08-31T10:54:13.239972Z","iopub.status.idle":"2021-08-31T11:00:17.686219Z","shell.execute_reply.started":"2021-08-31T10:54:13.23992Z","shell.execute_reply":"2021-08-31T11:00:17.684754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# modeling.blending(model='xgboost')\n# modeling.blending(model='lightgbm')\n# modeling.stacking(data_path='../input/kaggle30/no_cat2')\nmodeling.stacking(data_path='../input/kaggle30/full_dataset')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T10:40:10.771788Z","iopub.execute_input":"2021-08-31T10:40:10.772487Z","iopub.status.idle":"2021-08-31T10:46:41.098805Z","shell.execute_reply.started":"2021-08-31T10:40:10.772436Z","shell.execute_reply":"2021-08-31T10:46:41.097737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodeling.predict(data_path='../input/kaggle30/no_cat2')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T10:27:02.832034Z","iopub.execute_input":"2021-08-31T10:27:02.832393Z","iopub.status.idle":"2021-08-31T10:27:06.871292Z","shell.execute_reply.started":"2021-08-31T10:27:02.832357Z","shell.execute_reply":"2021-08-31T10:27:06.870386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#study = optuna.create_study(direction='minimize')\n#study.optimize(objective, n_trials=200) # set n_trials","metadata":{"execution":{"iopub.status.busy":"2021-08-31T09:33:01.006987Z","iopub.execute_input":"2021-08-31T09:33:01.007333Z","iopub.status.idle":"2021-08-31T09:33:01.015535Z","shell.execute_reply.started":"2021-08-31T09:33:01.007301Z","shell.execute_reply":"2021-08-31T09:33:01.012363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model):\n    '''Predict and generate submission.csv\n    '''\n    n_stackings = 3\n    # Define featurs\n    useful_features = [f\"pred_{i}\" for i in range(1, n_stackings+1)]\n\n    # Import datasets\n    data_path = '../input/kaggle30-2'\n    df_train = pd.read_csv('../input/30days-folds/train_folds.csv')\n    df_test = pd.read_csv('../input/30-days-of-ml/test.csv')\n    sample_submission = pd.read_csv('../input/30-days-of-ml/sample_submission.csv')\n    \n    # Join datasets on id\n    for i in range(n_stackings):\n        _df_valid = pd.read_csv(f'{data_path}/stacking_{i}_valid_pred.csv')\n        df_train = df_train.merge(_df_valid, on='id', how='left')\n        _df_test = pd.read_csv(f'{data_path}/stacking_{i}_test_pred.csv')\n        df_test = df_test.merge(_df_test, on='id', how='left')\n        \n    # Modeling\n    final_predictions = []\n    scores = []\n    for fold in range(5):\n        X_train =  df_train[df_train.kfold != fold].reset_index(drop=True)\n        X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n        X_test = df_test.copy()\n\n        y_train = X_train.target\n        y_valid = X_valid.target\n\n        X_train = X_train[useful_features]\n        X_valid = X_valid[useful_features]\n        X_test = X_test[useful_features]\n\n        # Modeling - Training\n        model.fit(\n            X_train, y_train, \n            early_stopping_rounds=300,\n            eval_set=[(X_valid, y_valid)],\n            verbose=False\n        )\n\n        # Modeling - Evaluation and Inference\n        valid_preds = model.predict(X_valid)\n        test_preds = model.predict(X_test)\n        final_predictions.append(test_preds)\n        rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n        print(f'Fold: {fold}, RMSE: {rmse}')\n        scores.append(rmse)\n\n    # Generate submission.csv\n    preds = np.mean(np.column_stack(final_predictions), axis=1)\n    preds = pd.DataFrame({'id': sample_submission.id, 'target': preds})\n    preds.to_csv('submission.csv', index=False)\n\n    print('Generate submission.csv succeed!')\n    print(f'Average RMSE: {np.mean(scores)}, STD of RMSE: {np.std(scores)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T18:23:03.949791Z","iopub.execute_input":"2021-08-31T18:23:03.950182Z","iopub.status.idle":"2021-08-31T18:23:03.964627Z","shell.execute_reply.started":"2021-08-31T18:23:03.950147Z","shell.execute_reply":"2021-08-31T18:23:03.963611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## Predictions Submission","metadata":{}},{"cell_type":"code","source":"# XGBoost Standardization (Single)\n# 0.7170814875570916, 0.71952\n\n# XGBoost Standardization\n# 0.7177871347165812, 0.71939\n\n# XGBoost Ordinal Encoding\n# 0.7177328219011401, 0.71936\n\n# XGBoost Ordinal Encoding, Full dataset\n# 0.7176631057759594, 0.71909","metadata":{},"execution_count":null,"outputs":[]}]}