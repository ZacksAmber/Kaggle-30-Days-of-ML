{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Statistics\nimport pandas as pd\nimport numpy as np\nimport math as mt\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# Data Preprocessing - Standardization, Encoding, Imputation\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\n\n\n# Data Preprocessing - Feature Engineering\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.decomposition import PCA\n\n# Data Preprocessing - ML Pipelines\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# ML - Modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# ML - Evaluation\nfrom sklearn.model_selection import cross_val_score\n\n# ML - Tuning\nfrom sklearn.model_selection import GridSearchCV\nimport optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-25T02:22:29.355231Z","iopub.execute_input":"2021-08-25T02:22:29.355563Z","iopub.status.idle":"2021-08-25T02:22:31.207877Z","shell.execute_reply.started":"2021-08-25T02:22:29.355531Z","shell.execute_reply":"2021-08-25T02:22:31.207066Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Read train dataset\ntrain_data = pd.read_csv('../input/30days-folds/train_folds.csv')\ntest_data = pd.read_csv('../input/30-days-of-ml/test.csv')\nsample_submission = pd.read_csv('../input/30-days-of-ml/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-25T02:22:31.209360Z","iopub.execute_input":"2021-08-25T02:22:31.209768Z","iopub.status.idle":"2021-08-25T02:22:34.372872Z","shell.execute_reply.started":"2021-08-25T02:22:31.209725Z","shell.execute_reply":"2021-08-25T02:22:34.372012Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%%time\n# target encoding\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\ntest_data = test_data[useful_features]\n\nfor col in cat_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(5):\n        X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n        X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n        feat = X_train.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        #print(feat)\n        X_valid.loc[:, f\"tar_enc_{col}\"] = X_valid[col].map(feat)\n        temp_df.append(X_valid)\n        if temp_test_feat is None:\n            temp_test_feat = test_data[col].map(feat)\n        else:\n            temp_test_feat += test_data[col].map(feat)\n    temp_test_feat /= 5\n    test_data.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    train_data = pd.concat(temp_df)\n    \nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if col.startswith(\"cat\")]\ntest_data = test_data[useful_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T02:22:34.374508Z","iopub.execute_input":"2021-08-25T02:22:34.374895Z","iopub.status.idle":"2021-08-25T02:22:41.204595Z","shell.execute_reply.started":"2021-08-25T02:22:34.374858Z","shell.execute_reply":"2021-08-25T02:22:41.203668Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"CPU times: user 6.06 s, sys: 730 ms, total: 6.79 s\nWall time: 6.82 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\ndef run(trial):\n    fold = 0\n    # Hyperparameters for Optuna\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n    \n    # Target Encoding\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    #X_test = test_data.copy()\n\n    y_train = X_train.target\n    y_valid = X_valid.target\n\n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n\n    # Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    #X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n\n    model = XGBRegressor(\n        random_state=42,\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        predictor=\"gpu_predictor\",\n        n_estimators=7000, # we have early_stopping_rounds\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth,\n    )\n    model.fit(X_train, y_train, \n              early_stopping_rounds=300, \n              eval_set=[(X_valid, y_valid)], verbose=1000)\n    preds_valid = model.predict(X_valid)\n    #test_preds = model.predict(X_test)\n    #final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    return rmse","metadata":{"execution":{"iopub.status.busy":"2021-08-25T02:23:42.533139Z","iopub.execute_input":"2021-08-25T02:23:42.533459Z","iopub.status.idle":"2021-08-25T02:23:42.542117Z","shell.execute_reply.started":"2021-08-25T02:23:42.533421Z","shell.execute_reply":"2021-08-25T02:23:42.540828Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"CPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 6.68 µs\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(run, n_trials=5) # set n_triasl","metadata":{"execution":{"iopub.status.busy":"2021-08-25T02:23:43.172106Z","iopub.execute_input":"2021-08-25T02:23:43.172421Z","iopub.status.idle":"2021-08-25T02:25:06.801786Z","shell.execute_reply.started":"2021-08-25T02:23:43.172390Z","shell.execute_reply":"2021-08-25T02:25:06.800667Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2021-08-25 02:23:43,173]\u001b[0m A new study created in memory with name: no-name-85e08a86-39bc-42cf-8002-3a7aae7a9600\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[0]\tvalidation_0-rmse:7.44694\n[1000]\tvalidation_0-rmse:0.71986\n[2000]\tvalidation_0-rmse:0.71900\n[2671]\tvalidation_0-rmse:0.71903\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2021-08-25 02:24:00,181]\u001b[0m Trial 0 finished with value: 0.7189675700875123 and parameters: {'learning_rate': 0.04322192958897396, 'reg_lambda': 2.0712359017603326e-07, 'reg_alpha': 43.117837539469534, 'subsample': 0.3907645562736698, 'colsample_bytree': 0.2329778388025192, 'max_depth': 6}. Best is trial 0 with value: 0.7189675700875123.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[0]\tvalidation_0-rmse:7.69807\n[1000]\tvalidation_0-rmse:0.73009\n[2000]\tvalidation_0-rmse:0.72562\n[3000]\tvalidation_0-rmse:0.72336\n[4000]\tvalidation_0-rmse:0.72187\n[5000]\tvalidation_0-rmse:0.72079\n[6000]\tvalidation_0-rmse:0.72001\n[6999]\tvalidation_0-rmse:0.71943\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2021-08-25 02:24:22,223]\u001b[0m Trial 1 finished with value: 0.7194267651939508 and parameters: {'learning_rate': 0.010611982159756318, 'reg_lambda': 1.4024820263697298e-06, 'reg_alpha': 2.4850247913612537e-07, 'subsample': 0.7808792881961223, 'colsample_bytree': 0.19900067122025827, 'max_depth': 3}. Best is trial 0 with value: 0.7189675700875123.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[0]\tvalidation_0-rmse:6.43055\n[1000]\tvalidation_0-rmse:0.72598\n[2000]\tvalidation_0-rmse:0.72330\n[3000]\tvalidation_0-rmse:0.72210\n[4000]\tvalidation_0-rmse:0.72148\n[5000]\tvalidation_0-rmse:0.72099\n[5908]\tvalidation_0-rmse:0.72067\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2021-08-25 02:24:35,086]\u001b[0m Trial 2 finished with value: 0.7205383847882317 and parameters: {'learning_rate': 0.17531046557770022, 'reg_lambda': 0.00011200018229513348, 'reg_alpha': 6.296378980490636, 'subsample': 0.12074293373810374, 'colsample_bytree': 0.2172649214085463, 'max_depth': 1}. Best is trial 0 with value: 0.7189675700875123.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[0]\tvalidation_0-rmse:7.20646\n[1000]\tvalidation_0-rmse:0.72995\n[2000]\tvalidation_0-rmse:0.72747\n[3000]\tvalidation_0-rmse:0.72599\n[4000]\tvalidation_0-rmse:0.72497\n[5000]\tvalidation_0-rmse:0.72418\n[6000]\tvalidation_0-rmse:0.72346\n[6999]\tvalidation_0-rmse:0.72289\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2021-08-25 02:24:50,898]\u001b[0m Trial 3 finished with value: 0.7228869231654257 and parameters: {'learning_rate': 0.07443694940201989, 'reg_lambda': 3.12715674497084e-07, 'reg_alpha': 29.396597265086125, 'subsample': 0.5355849263649969, 'colsample_bytree': 0.4697687412835221, 'max_depth': 1}. Best is trial 0 with value: 0.7189675700875123.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[0]\tvalidation_0-rmse:6.89455\n[1000]\tvalidation_0-rmse:0.72786\n[2000]\tvalidation_0-rmse:0.72519\n[3000]\tvalidation_0-rmse:0.72366\n[4000]\tvalidation_0-rmse:0.72263\n[5000]\tvalidation_0-rmse:0.72191\n[6000]\tvalidation_0-rmse:0.72140\n[6999]\tvalidation_0-rmse:0.72115\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2021-08-25 02:25:06,795]\u001b[0m Trial 4 finished with value: 0.7210970603521394 and parameters: {'learning_rate': 0.11495813555984329, 'reg_lambda': 0.00047121305240065055, 'reg_alpha': 3.332074101256634e-05, 'subsample': 0.4724938177882402, 'colsample_bytree': 0.6755355165449973, 'max_depth': 1}. Best is trial 0 with value: 0.7189675700875123.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 1min 23s, sys: 994 ms, total: 1min 24s\nWall time: 1min 23s\n","output_type":"stream"}]},{"cell_type":"code","source":"study.best_params","metadata":{"execution":{"iopub.status.busy":"2021-08-25T02:25:13.396534Z","iopub.execute_input":"2021-08-25T02:25:13.396868Z","iopub.status.idle":"2021-08-25T02:25:13.404827Z","shell.execute_reply.started":"2021-08-25T02:25:13.396835Z","shell.execute_reply":"2021-08-25T02:25:13.403959Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'learning_rate': 0.04322192958897396,\n 'reg_lambda': 2.0712359017603326e-07,\n 'reg_alpha': 43.117837539469534,\n 'subsample': 0.3907645562736698,\n 'colsample_bytree': 0.2329778388025192,\n 'max_depth': 6}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# With Standardization + Normalization\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n\n    # Preprocessing - Standardization\n    scaler = StandardScaler()\n    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n    X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n    X_test[num_cols] = scaler.transform(X_test[num_cols]) # Q. The last transform\n    \n    # Preprocessing - Normalizatino\n    normalizer = Normalizer()\n    X_train[num_cols] = normalizer.fit_transform(X_train[num_cols])\n    X_valid[num_cols] = normalizer.transform(X_valid[num_cols])\n    X_test[num_cols] = normalizer.transform(X_test[num_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:44:15.294985Z","iopub.execute_input":"2021-08-24T21:44:15.295451Z","iopub.status.idle":"2021-08-24T21:44:38.737812Z","shell.execute_reply.started":"2021-08-24T21:44:15.295406Z","shell.execute_reply":"2021-08-24T21:44:38.736862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# With Standardization\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Preprocessing - Standardization\n    scaler = StandardScaler()\n    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n    X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n    X_test[num_cols] = scaler.transform(X_test[num_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Log transformation + Tuning\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfor col in num_cols:\n    train_data[col] = np.log1p(train_data[col])\n    test_data[col] = np.log1p(test_data[col])\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nprint('You need to reset dataframe!')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:47:08.741473Z","iopub.execute_input":"2021-08-24T21:47:08.741828Z","iopub.status.idle":"2021-08-24T21:47:37.645747Z","shell.execute_reply.started":"2021-08-24T21:47:08.741796Z","shell.execute_reply":"2021-08-24T21:47:37.644785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# polynomial features + Tuning\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\npoly = PolynomialFeatures(degree=2, \n                          interaction_only=True, # If true, only interaction features are produced: features that are products of at most degree distinct input features (so not x[1] ** 2, x[0] * x[2] ** 3, etc.).\n                          include_bias=False)\ntrain_poly = poly.fit_transform(train_data[num_cols])\ntest_poly = poly.fit_transform(test_data[num_cols])\n\ndf_train_poly = pd.DataFrame(train_poly, columns=[f\"poly_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"poly_{i}\" for i in range(test_poly.shape[1])])\n\ntrain_data = pd.concat([train_data, df_train_poly], axis=1)\ntest_data = pd.concat([test_data, df_test_poly], axis=1)\n\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nprint('You need to reset dataframe!')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T22:20:02.277023Z","iopub.execute_input":"2021-08-24T22:20:02.27741Z","iopub.status.idle":"2021-08-24T22:20:55.762472Z","shell.execute_reply.started":"2021-08-24T22:20:02.277378Z","shell.execute_reply":"2021-08-24T22:20:55.761626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2021-08-24T22:29:42.845424Z","iopub.execute_input":"2021-08-24T22:29:42.845783Z","iopub.status.idle":"2021-08-24T22:29:42.932702Z","shell.execute_reply.started":"2021-08-24T22:29:42.845746Z","shell.execute_reply":"2021-08-24T22:29:42.931612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# One-Hot Encoding + Ordinal Encoding + Tuning\n# pd.cut \n# Model Tuning + drop cat2\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\noe_cols = ['cat9']\nohe_cols = cat_cols\nohe_cols.remove('cat9')\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[oe_cols] = ordinal_encoder.fit_transform(X_train[oe_cols])\n    X_valid[oe_cols] = ordinal_encoder.transform(X_valid[oe_cols])\n    X_test[oe_cols] = ordinal_encoder.transform(X_test[oe_cols]) # Q. The last transform\n    \n    # Preprocessing - One-Hot Encoding\n    ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    X_train_ohe = ohe.fit_transform(X_train[ohe_cols])\n    X_valid_ohe = ohe.transform(X_valid[ohe_cols])\n    X_test_ohe = ohe.transform(X_test[ohe_cols]) # Q. The last transform\n    \n    X_train_ohe = pd.DataFrame(X_train_ohe, columns=[f\"ohe_{i}\" for i in range(X_train_ohe.shape[1])])\n    X_valid_ohe = pd.DataFrame(X_valid_ohe, columns=[f\"ohe_{i}\" for i in range(X_valid_ohe.shape[1])])\n    X_test_ohe = pd.DataFrame(X_test_ohe, columns=[f\"ohe_{i}\" for i in range(X_test_ohe.shape[1])])\n    \n    X_train = pd.concat([X_train.drop(columns=ohe_cols), X_train_ohe], axis=1)\n    X_valid = pd.concat([X_valid.drop(columns=ohe_cols), X_valid_ohe], axis=1)\n    X_test = pd.concat([X_test.drop(columns=ohe_cols), X_test_ohe], axis=1)\n\n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nprint('You need to reset dataframe!')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T22:42:37.746539Z","iopub.execute_input":"2021-08-24T22:42:37.746908Z","iopub.status.idle":"2021-08-24T22:43:05.859597Z","shell.execute_reply.started":"2021-08-24T22:42:37.746864Z","shell.execute_reply":"2021-08-24T22:43:05.857667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Model Tuning + drop cat2\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\", \"cat2\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    #model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nprint('You need to reset dataframe!')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:54:06.321382Z","iopub.execute_input":"2021-08-24T21:54:06.321743Z","iopub.status.idle":"2021-08-24T21:54:33.276709Z","shell.execute_reply.started":"2021-08-24T21:54:06.321709Z","shell.execute_reply":"2021-08-24T21:54:33.275794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Model Tuning + drop cat2, cat6\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\", \"cat2\", \"cat6\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    #model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nprint('You need to reset dataframe!')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:55:30.031254Z","iopub.execute_input":"2021-08-24T21:55:30.031587Z","iopub.status.idle":"2021-08-24T21:55:54.609825Z","shell.execute_reply.started":"2021-08-24T21:55:30.031558Z","shell.execute_reply":"2021-08-24T21:55:54.608783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Tuning + Standardization\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Preprocessing - Standardization\n    scaler = StandardScaler()\n    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n    X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n    X_test[num_cols] = scaler.transform(X_test[num_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    #model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T22:31:05.89861Z","iopub.execute_input":"2021-08-24T22:31:05.898981Z","iopub.status.idle":"2021-08-24T22:31:33.912625Z","shell.execute_reply.started":"2021-08-24T22:31:05.898945Z","shell.execute_reply":"2021-08-24T22:31:33.911724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Only Model Tuning\nuseful_features = [col for col in train_data.columns if col not in (\"id\", \"target\", \"kfold\")]\ncat_cols = [col for col in useful_features if \"cat\" in col]\nnum_cols = [col for col in useful_features if col.startswith('cont')]\ntest_data = test_data[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    # Preprocessing - Kfold\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    X_test = test_data.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    # Preprocessing - Ordinal Encoding\n    ordinal_encoder = OrdinalEncoder()\n    X_train[cat_cols] = ordinal_encoder.fit_transform(X_train[cat_cols])\n    X_valid[cat_cols] = ordinal_encoder.transform(X_valid[cat_cols])\n    X_test[cat_cols] = ordinal_encoder.transform(X_test[cat_cols]) # Q. The last transform\n    \n    # Training\n    #model = RandomForestRegressor(random_state=fold, n_jobs=-1)\n    #model = XGBRegressor(random_state=fold, n_jobs=8)\n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor', \n                         learning_rate=0.1, n_estimators=1000, max_depth=3, colsample_bytree=0.3)\n    model.fit(X_train, y_train)\n    \n    # Evaluation\n    preds_valid = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    \n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:50:43.55151Z","iopub.execute_input":"2021-08-24T21:50:43.551835Z","iopub.status.idle":"2021-08-24T21:51:12.086803Z","shell.execute_reply.started":"2021-08-24T21:50:43.551803Z","shell.execute_reply":"2021-08-24T21:51:12.085859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Export submission.csv\npreds = np.mean(np.column_stack(final_predictions), axis=1)\npreds = pd.DataFrame({'id': sample_submission.id, 'target': preds})\npreds.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T21:48:43.959974Z","iopub.execute_input":"2021-08-24T21:48:43.960328Z","iopub.status.idle":"2021-08-24T21:48:44.474981Z","shell.execute_reply.started":"2021-08-24T21:48:43.960296Z","shell.execute_reply":"2021-08-24T21:48:44.474102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}